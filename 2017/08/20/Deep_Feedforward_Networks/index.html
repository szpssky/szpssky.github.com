<!DOCTYPE html><html class="theme-next pisces use-motion" lang="zh-Hans"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><meta name="theme-color" content="#222"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css"><link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css"><link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css"><link href="/css/main.css?v=5.1.2" rel="stylesheet" type="text/css"><meta name="keywords" content="Deep Learning,"><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.2"><meta name="description" content="深度前馈网络也叫作前馈神经网络或者多层感知机，一种理解前馈网络的方式是从线性模型开始的，并考虑克服其局限性。线性模型，如逻辑回归和线性回归，无论是通过闭解形式还是使用凸优化，它们都能高效且可靠的拟合。但是也存在明显缺陷，线性模型被局限在线性函数里，所以无法理解两个输入变量的相互作用。 为了扩展线性模型来表示\(x\)的非线性函数，引入非线性变换\(\phi(x)\)，关于如何选择\(\phi\)，"><meta name="keywords" content="Deep Learning"><meta property="og:type" content="article"><meta property="og:title" content="《Deep Learning》笔记 —— 深度前馈网络"><meta property="og:url" content="http://blog.tifosi-m.com/2017/08/20/Deep_Feedforward_Networks/index.html"><meta property="og:site_name" content="Think Different"><meta property="og:description" content="深度前馈网络也叫作前馈神经网络或者多层感知机，一种理解前馈网络的方式是从线性模型开始的，并考虑克服其局限性。线性模型，如逻辑回归和线性回归，无论是通过闭解形式还是使用凸优化，它们都能高效且可靠的拟合。但是也存在明显缺陷，线性模型被局限在线性函数里，所以无法理解两个输入变量的相互作用。 为了扩展线性模型来表示\(x\)的非线性函数，引入非线性变换\(\phi(x)\)，关于如何选择\(\phi\)，"><meta property="og:locale" content="zh-Hans"><meta property="og:image" content="http://blog.tifosi-m.com/2017/08/20/Deep_Feedforward_Networks/forward.png"><meta property="og:image" content="http://blog.tifosi-m.com/2017/08/20/Deep_Feedforward_Networks/backward.png"><meta property="og:updated_time" content="2017-09-29T04:58:45.351Z"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="《Deep Learning》笔记 —— 深度前馈网络"><meta name="twitter:description" content="深度前馈网络也叫作前馈神经网络或者多层感知机，一种理解前馈网络的方式是从线性模型开始的，并考虑克服其局限性。线性模型，如逻辑回归和线性回归，无论是通过闭解形式还是使用凸优化，它们都能高效且可靠的拟合。但是也存在明显缺陷，线性模型被局限在线性函数里，所以无法理解两个输入变量的相互作用。 为了扩展线性模型来表示\(x\)的非线性函数，引入非线性变换\(\phi(x)\)，关于如何选择\(\phi\)，"><meta name="twitter:image" content="http://blog.tifosi-m.com/2017/08/20/Deep_Feedforward_Networks/forward.png"><script type="text/javascript" id="hexo.configurations">var NexT=window.NexT||{},CONFIG={root:"/",scheme:"Pisces",sidebar:{position:"left",display:"post",offset:12,offset_float:12,b2t:!1,scrollpercent:!1,onmobile:!1},fancybox:!0,tabs:!0,motion:!0,duoshuo:{userId:"0",author:"博主"},algolia:{applicationID:"FE4Z1ZLHES",apiKey:"1d320de7cd3f274c9be1be5bf7d93ad4",indexName:"my-hexo-blog",hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}}}</script><link rel="canonical" href="http://blog.tifosi-m.com/2017/08/20/Deep_Feedforward_Networks/"><title>《Deep Learning》笔记 —— 深度前馈网络 | Think Different</title><script type="text/javascript">var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?72a53052e7dd73ad9a365edd47e443cd";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script></head><body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans"><div class="container sidebar-position-left page-post-detail"><div class="headband"></div><header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-wrapper"><div class="site-meta"><div class="custom-logo-site-title"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">Think Different</span><span class="logo-line-after"><i></i></span></a></div><p class="site-subtitle"></p></div><div class="site-nav-toggle"> <button><span class="btn-bar"></span><span class="btn-bar"></span><span class="btn-bar"></span></button></div></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i><br> 首页</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i><br> 分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i><br> 归档</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i><br> 标签</a></li><li class="menu-item menu-item-search"><a href="javascript:;" class="popup-trigger"><i class="menu-item-icon fa fa-search fa-fw"></i><br> 搜索</a></li></ul><div class="site-search"><div class="algolia-popup popup search-popup"><div class="algolia-search"><div class="algolia-search-input-icon"><i class="fa fa-search"></i></div><div class="algolia-search-input" id="algolia-search-input"></div></div><div class="algolia-results"><div id="algolia-stats"></div><div id="algolia-hits"></div><div id="algolia-pagination" class="algolia-pagination"></div></div><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span></div></div></nav></div></header><main id="main" class="main"><div class="main-inner"><div class="content-wrap"><div id="content" class="content"><div id="posts" class="posts-expand"><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><div class="post-block"><link itemprop="mainEntityOfPage" href="http://blog.tifosi-m.com/2017/08/20/Deep_Feedforward_Networks/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="Zhipeng Shen"><meta itemprop="description" content=""><meta itemprop="image" content="/images/gravatar.jpg"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Think Different"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">《Deep Learning》笔记 —— 深度前馈网络</h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i></span> <span class="post-meta-item-text">发表于</span> <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-08-20T13:00:00+00:00">2017-08-20</time></span> <span class="post-category"><span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-folder-o"></i></span> <span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/Deep-Learning-Book/" itemprop="url" rel="index"><span itemprop="name">Deep Learning Book</span></a></span></span> <span id="/2017/08/20/Deep_Feedforward_Networks/" class="leancloud_visitors" data-flag-title="《Deep Learning》笔记 —— 深度前馈网络"><span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-eye"></i></span> <span class="post-meta-item-text">阅读次数&#58;</span><span class="leancloud-visitors-count"></span></span></div></header><div class="post-body" itemprop="articleBody"><p>深度前馈网络也叫作前馈神经网络或者多层感知机，一种理解前馈网络的方式是从线性模型开始的，并考虑克服其局限性。线性模型，如逻辑回归和线性回归，无论是通过闭解形式还是使用凸优化，它们都能高效且可靠的拟合。但是也存在明显缺陷，线性模型被局限在线性函数里，所以无法理解两个输入变量的相互作用。</p><p>为了扩展线性模型来表示<span class="math">\(x\)</span>的非线性函数，引入非线性变换<span class="math">\(\phi(x)\)</span>，关于如何选择<span class="math">\(\phi\)</span>，有如下方法：</p><p>1）使用一个通用<span class="math">\(\phi\)</span>，例如无限维的<span class="math">\(\phi\)</span>，它隐含地用在基于RBF核的核机器上。非常通用的特征映射通常只基于局部光滑原则，并且没有足够先验信息解决高级问题。</p><p>2）手动设计<span class="math">\(\phi\)</span>，缺点是不同的领域难以迁移。</p><p>3）深度学习的策略是自主学习<span class="math">\(\phi\)</span>，这种方法的优点是我们只需要寻找正确的函数族就可以了，不需要去寻找精确函数。<a id="more"></a></p><h3 id="基于梯度的学习">1. 基于梯度的学习</h3><p>线性模型和神经网络最大的区别在于神经网络的非线性导致大多数我们感兴趣的代价函数都变得非凸。这意味着神经网络的训练通常使用迭代的、基于梯度的优化，仅仅使得代价函数达到一个非常小的值，而不是想训练线性回归模型、SVM的凸优化那样保证全局收敛。</p><h4 id="代价函数">1.1 代价函数</h4><p>大多数现代神经网络使用最大似然来训练，这意味着代价函数是负对数似然，它与训练数据和模型分布间的交叉熵等价，这个代价函数表示为： <span class="math">\[J(\theta)=-\mathbb{E}_{x,y \sim \hat{p}_{data}}log\,p_{model}(y|x)\]</span></p><p>使用最大似然来导出代价函数的方法的一个优势是，它减轻了为每个模型设计代价函数的负担。明确一个模型的<span class="math">\(p(y|x)\)</span>则自动地确定了一个代价函数<span class="math">\(log\,p(y|x)\)</span>。</p><p>特别的，代价函数的梯度必须足够的大和具有足够的预测性，而饱和的函数把梯度变得非常小破坏了这个目标。值得注意的是这种情况在神经网络中经常发生，因为隐藏层或输出层的激活函数会饱和。负对数似然能够避免这个问题，因为负对数代价函数中的对数函数消除了某些输出单元的指数效果。</p><h4 id="输出单元">1.2 输出单元</h4><h5 id="高斯输出分布的线性单元">高斯输出分布的线性单元</h5><p>给定特征<span class="math">\(h\)</span>，线性输出单元产生一个向量<span class="math">\(\hat{y}=W^Th+b\)</span>。线性输出层经常用来产生条件高斯分布的均值： <span class="math">\[p(y|x)=\mathcal{N}(y;\hat{y},I)\]</span> 最大化其对数似然等价于最小化均方误差。</p><p>因为线性单元不会饱和，所以易于采用基于梯度的优化算法。</p><h5 id="伯努利输出分布的sigmoid单元">伯努利输出分布的sigmoid单元</h5><p>sigmoid输出单元定义为<span class="math">\(\hat{y}=\sigma(w^Th+b)\)</span>，可将其分为两个部分，使用线性层来计算<span class="math">\(z=w^Th+b\)</span>，接着使用sigmoid激活函数将<span class="math">\(z\)</span>转化为概率。</p><p>当我们使用其他损失函数，损失函数将在<span class="math">\(\sigma(z)\)</span>饱和时饱和，sigmoid在<span class="math">\(z\)</span>取非常小的负值时会饱和到0，取非常大的正值时会饱和到1，这种情况一旦发生，梯度会变得非常小以至于不能用来学习，因此最大似然几乎总是训练sigmoid输出单元的优选方案。</p><h5 id="多项式输出分布的softmax单元">多项式输出分布的softmax单元</h5><p>任何时候，当需要表示一个具有n个可能取值的离散随机变量的分布时，都可以使用softmax函数，softmax函数最常用作分类器的输出，来表示n个不同类上的概率分布。用于伯努利分布的方法可以推广到多项式分布。首先线性层预测未归一化的对数概率： <span class="math">\[z=W^Th+b\]</span> 其中<span class="math">\(z_i=log\,\hat{P}(y=i|x)\)</span>，softmax函数对z指数化和归一化来获得需要的<span class="math">\(\hat{y}\)</span>，其函数形式为： <span class="math">\[softmax(z)_i=\frac{e^{z_i}}{\sum_je^{z_j}}\]</span> 使用最大似然训练softmax输出目标y，对数似然中的log将抵消softmax中的<span class="math">\(e\)</span>： <span class="math">\[log\,softmax(z_i)=z_i-log\,\sum_{j}e^{z_j}\]</span> 该式中的第一项<span class="math">\(z_i\)</span>不会饱和，所以即使<span class="math">\(z_i\)</span>对第二项贡献很小，学习依然可以进行。此外第二项<span class="math">\(log\,\sum_je^{z_j}\)</span>可以近似为<span class="math">\(\max\limits_j\,z_j\)</span>，如果正确答案已经具有softmax的最大输入则<span class="math">\(-z_i\)</span>与<span class="math">\(log\,\sum_j \approx \max\limits_j\,z_j=z_j\)</span>将大致抵消。这样整个样本对整体训练代价贡献很小，所以这个代价将主要由为被正确分类的样本产生。</p><p>对数似然之外的许多目标函数对softmax函数不起作用，因为指数函数的变量取非常小的负值时将造成梯度消失，特别是平方误差，对softmax单元来说它是很差的损失函数。</p><h5 id="其他的输出类型">其他的输出类型</h5><p>最大似然原则给如何为任何种类的输出层设计一个好的代价函数提供了指导。</p><h3 id="隐藏单元">2. 隐藏单元</h3><p>ReLU单元是隐藏单元极好的默认选择，一些隐藏层单元可能并不是在所有的输入点上都是可微的，例如ReLU单元<span class="math">\(g(z)=max{0,z}\)</span>在<span class="math">\(z=0\)</span>处不可微，在实践中，梯度下降对这些机器学习模型仍然表现良好，部分原因是因为神经网络算法通常不会达到代价函数的局部最小值，而是仅仅显著减小它的值。</p><h4 id="relu单元的扩展">2.1 ReLU单元的扩展</h4><p>ReLU单元易于优化，因为其与线性单元非常类似，ReLU单元在其一半的定义域上输出为零，这使得ReLU单元只要处于激活状态，其导数都比较大和一致，而且二阶导数几乎处处为0，处于激活状态时候，它的一阶导数处处为1，所以它的梯度方向对学习来说更有用。</p><p>ReLU的一个缺陷是不能通过基于梯度的方法学习那些使他它们激活为0的样本，而ReLU的各自扩展则保证了能在各个位置都接受到梯度。</p><p>ReLU的三个扩展基于当<span class="math">\(z_i&lt;0\)</span>时使用一个非零的斜率<span class="math">\(\alpha_i\)</span>:<span class="math">\(h_i=g(z,\alpha)_i=max(0,z_i)+\alpha_imin(0,z_i)\)</span>。</p><p>1）绝对值ReLU，固定<span class="math">\(\alpha_i=-1\)</span>来得到<span class="math">\(g(z)=|z|\)</span>，它用于图像中的对象识别，在寻找输入照明极性反转下不变的特征。</p><p>2）渗漏ReLU(Leaky ReLU)，将<span class="math">\(\alpha_i\)</span>固定为一个类似0.01的小值。</p><p>3）参数化ReLU(PReLU)，将<span class="math">\(\alpha_i\)</span>作为一个可学习的参数。</p><p>maxout单元，进一步扩展了ReLU，其将<span class="math">\(z\)</span>划分为每组具有k个值得组： <span class="math">\[ g(z)_i=\max_{j\in\mathbb(G^i)}z_j \]</span> 每个maxout单元输出每组中的最大元素。这提供了一种方法来学习对输入x空间中的多个方向的响应的分段线性函数。maxout单元可以学习多大k段的分段线性凸函数，因此可视为学习激活函数本身。使用足够大的k，maxout单元可以以任意的精确度来近似任何凸函数。(此部分详见<a href="https://arxiv.org/abs/1302.4389" target="_blank" rel="external">paper</a>)</p><h4 id="logistic-sigmoid与双曲正切函数">2.2 logistic sigmoid与双曲正切函数</h4><p>sigmoid单元的广泛饱和性会使基于梯度的学习变得非常困难，所以现在不鼓励使用sigmoid作为前馈网络的隐藏层单元。如果必须要使用sigmoid作为激活函数，则双曲正切函数通常要比logistic sigmoid函数表现的更好，在<span class="math">\(tanh(0)=0\)</span>而<span class="math">\(\sigma(0)=\frac{1}{2}\)</span>的意义上，tanh更像单位函数(在0附近)，所以只要网络的激活保持地很小，训练深层网络类似于训练一个线性模型。</p><p>sigmoid激活函数在除前馈神经网络以外的情景中更为常见，循环神经网络、许多概率模型以及一些自编码器有一些额外的要求使得其不能使用分段激活函数。</p><h4 id="其他隐藏层单元">2.3 其他隐藏层单元</h4><p>1）线性隐藏单元，即完全没有使用激活函数，线性隐藏层单元提供了一种减速网络中参数数量的有效方法。</p><p>2）softmax单元，softmax单元表示具有k个可能值得离散型变量的概率分布，所以它们可以作为一种开关。</p><p>3）RBF函数，<span class="math">\(h_i = e^{-\frac{1}{\sigma_i^2}||W_{:,i}-x||^2}\)</span>因对大多数x饱和到0，所以很难优化。</p><p>4）softplus函数，<span class="math">\(g(a)=\zeta(a)=log(1+e^a)\)</span>，ReLU的平滑版本，通常不鼓励使用softplus，通常ReLU表现更好。</p><p>5）硬双曲正切函数，<span class="math">\(g(a)=\max(-1,min(1,a))\)</span>，形状与tanh类似，但相比于双曲正切，hard tanh是有界的。</p><h3 id="架构设计">3. 架构设计</h3><p>架构指网络的整体结构，具有多少个单元以及这些单元如何连接。</p><h4 id="万能近似">3.1 万能近似</h4><p>具有隐藏层的前馈网络提供了一种万能近似框架，具体的来说，万能定理表明，一个前馈近似神经网络如果具有线性输出层和至少一层具有任何一种“挤压”性质的激活函数的隐藏层，只要给予网络足够数量的隐藏单元，则它可以以任意精度来近似任何从一个有限维空间到另一个有限维空间的Borel可测函数（在<span class="math">\(\mathbb(R)^n\)</span>的有界闭集上的任意连续函数时Borel可测的）。神经网络也可以近似从任何有限维离散空间映射到另一个任意函数。</p><p>万能近似定理意味着无论试图学习什么样的函数，都能通过一个足够大MLP来表示这个函数，但是不能保证训练算法一定能够学到这个函数。存在两个原因可能导致学习失败，首先，用于训练的优化算法可能找不到用于期望函数的参数值。其次，训练算法可能由于过拟合而选择了错误的函数。</p><p>总之，具有单层前馈网络足以表示任何函数，但是网络层可能大得不可实现，并且可能无法正确地学习和泛化。在很多情况下使用更深的模型能够减少表示期望函数所需的单元数量，并且减少泛化误差。</p><h4 id="其他架构上的考虑">3.2 其他架构上的考虑</h4><p>许多神经网络架构被开发用于特定的任务，用于计算机视觉的卷积神经网络，用于序列处理的循环神经网络等。一般来说，层不需要连接在链中，但许多架构构建了一个主链，随后又添加了额外的架构特性，例如从层i到i+2或者更高层的跳跃连接，这些跳跃连接使得梯度更容易从输出层流向更接近输入的层。</p><h3 id="反向传播">4. 反向传播</h3><h4 id="微积分中的链式法则">微积分中的链式法则</h4><p>设<span class="math">\(x\)</span>是实数，<span class="math">\(f\text{和}g\)</span>是从实数映射到实数的函数，假设<span class="math">\(y=g(x)\)</span>并且<span class="math">\(z=f(g(x))=f(y)\)</span>，那么链式法则是： <span class="math">\[ \frac{dz}{dx}=\frac{dz}{dy}\frac{dy}{dx} \]</span> 扩展到向量形式，<span class="math">\(x\in \mathbb{R}^m\)</span>，<span class="math">\(y\in \mathbb{R}^n\)</span>，<span class="math">\(g\)</span>是从<span class="math">\(\mathbb{R}^m\)</span>到<span class="math">\(\mathbb{R}^n\)</span>的映射，<span class="math">\(f\)</span>是从<span class="math">\(\mathbb{R}^n\)</span>到<span class="math">\(\mathbb{R}\)</span>的映射，如果<span class="math">\(y=g(x)\)</span>，且<span class="math">\(z=f(y)\)</span>，则 <span class="math">\[ \frac{\partial z}{\partial x_i}=\sum_j \frac{\partial z}{\partial y_j}\frac{\partial y_j}{\partial x_i} \]</span> 向量记法： <span class="math">\[ \nabla_xz = (\frac{\partial z}{\partial x})^T\nabla_yz \]</span> 其中<span class="math">\(\frac{\partial y}{\partial x}\)</span>是<span class="math">\(g\)</span>的<span class="math">\(n\times m\)</span>Jacobian矩阵。</p><p>扩展到张量，使用<span class="math">\(\nabla_\mathbf{X}z\)</span>表示值<span class="math">\(z\)</span>关于张量<span class="math">\(\mathbf{X}\)</span>的梯度，使用单个变量<span class="math">\(i\)</span>来表示张量完整的索引组，令<span class="math">\(\mathbf{Y}=g(\mathbf{X})\)</span>且<span class="math">\(z=f(\mathbf{Y})\)</span>，则 <span class="math">\[ \nabla_\mathbf{X}z=\sum_j(\nabla_\mathbf{X}Y_j)\frac{\partial z}{\partial \mathbf{Y}_j} \]</span></p><h4 id="递归的使用链式法则实现反向传播">递归的使用链式法则实现反向传播</h4><p>在迭代的计算工程中存在两种策略，存储子表达式还是重新进行计算。例如，令<span class="math">\(w\in \mathbb{R}\)</span>为计算图的的输入，每一步使用相同的操作函数<span class="math">\(f:\mathbb{R} \rightarrow \mathbb{R}\)</span>，即<span class="math">\(x=f(w)\)</span>，<span class="math">\(y=f(x)\)</span>，<span class="math">\(z=f(y)\)</span>，为了计算<span class="math">\(\frac{\partial z}{\partial w}\)</span>得到： <span class="math">\[ \begin{array}{} &amp; \quad \frac{\partial z}{\partial w} \\ &amp; = \frac{\partial z}{\partial y} \frac{\partial y}{\partial x} \frac{\partial x}{\partial w} \\ &amp; = f&#39;(y)f&#39;(x)f&#39;(w) \end{array} \]</span> 此实现方法，仅计算<span class="math">\(f(w)\)</span>的值一次并将它存储在变量<span class="math">\(x\)</span>中。另一种策略则使用<span class="math">\(f&#39;(f(f(w)))f&#39;(f(w))f&#39;(w)\)</span>，通常在存储受限时会使用。</p><h4 id="mlp中的的反向传播计算">MLP中的的反向传播计算</h4><p>前向传播： <img src="/2017/08/20/Deep_Feedforward_Networks/forward.png" alt="forward.png" title=""></p><p>反向传播： <img src="/2017/08/20/Deep_Feedforward_Networks/backward.png" alt="backward.png" title=""></p></div><footer class="post-footer"><div class="post-tags"> <a href="/tags/Deep-Learning/" rel="tag"># Deep Learning</a></div><div class="post-nav"><div class="post-nav-next post-nav-item"><a href="/2017/08/19/Challenges Motivating Deep Learning/" rel="next" title="《Deep Learning》笔记 —— 促使深度学习发展的原因"><i class="fa fa-chevron-left"></i> 《Deep Learning》笔记 —— 促使深度学习发展的原因</a></div><span class="post-nav-divider"></span><div class="post-nav-prev post-nav-item"> <a href="/2017/08/22/ML_linear model/" rel="prev" title="《机器学习》笔记 —— 线性模型">《机器学习》笔记 —— 线性模型<i class="fa fa-chevron-right"></i></a></div></div></footer></div></article><div class="post-spread"></div></div></div><div class="comments" id="comments"></div></div><div class="sidebar-toggle"><div class="sidebar-toggle-line-wrap"><span class="sidebar-toggle-line sidebar-toggle-line-first"></span><span class="sidebar-toggle-line sidebar-toggle-line-middle"></span><span class="sidebar-toggle-line sidebar-toggle-line-last"></span></div></div><aside id="sidebar" class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap"> 文章目录</li><li class="sidebar-nav-overview" data-target="site-overview"> 站点概览</li></ul><section class="site-overview sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"> <img class="site-author-image" itemprop="image" src="/images/gravatar.jpg" alt="Zhipeng Shen"><p class="site-author-name" itemprop="name">Zhipeng Shen</p><p class="site-description motion-element" itemprop="description"></p></div><nav class="site-state motion-element"><div class="site-state-item site-state-posts"> <a href="/archives/"><span class="site-state-item-count">18</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"> <a href="/categories/index.html"><span class="site-state-item-count">9</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"> <a href="/tags/index.html"><span class="site-state-item-count">10</span> <span class="site-state-item-name">标签</span></a></div></nav><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/szpssky" target="_blank" title="GitHub"><i class="fa fa-fw fa-github"></i> GitHub</a></span><span class="links-of-author-item"><a href="http://weibo.com/lovelss310/" target="_blank" title="Weibo"><i class="fa fa-fw fa-weibo"></i> Weibo</a></span><span class="links-of-author-item"><a href="http://www.linkedin.com/in/szpssky" target="_blank" title="Linkedin"><i class="fa fa-fw fa-linkedin"></i> Linkedin</a></span><span class="links-of-author-item"><a href="mailto:szpssky@gmail.com" target="_blank" title="E-Mail"><i class="fa fa-fw fa-envelope"></i> E-Mail</a></span></div></section><section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active"><div class="post-toc"><div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#基于梯度的学习"><span class="nav-number">1.</span> <span class="nav-text">1. 基于梯度的学习</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#代价函数"><span class="nav-number">1.1.</span> <span class="nav-text">1.1 代价函数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#输出单元"><span class="nav-number">1.2.</span> <span class="nav-text">1.2 输出单元</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#高斯输出分布的线性单元"><span class="nav-number">1.2.1.</span> <span class="nav-text">高斯输出分布的线性单元</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#伯努利输出分布的sigmoid单元"><span class="nav-number">1.2.2.</span> <span class="nav-text">伯努利输出分布的sigmoid单元</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#多项式输出分布的softmax单元"><span class="nav-number">1.2.3.</span> <span class="nav-text">多项式输出分布的softmax单元</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#其他的输出类型"><span class="nav-number">1.2.4.</span> <span class="nav-text">其他的输出类型</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#隐藏单元"><span class="nav-number">2.</span> <span class="nav-text">2. 隐藏单元</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#relu单元的扩展"><span class="nav-number">2.1.</span> <span class="nav-text">2.1 ReLU单元的扩展</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#logistic-sigmoid与双曲正切函数"><span class="nav-number">2.2.</span> <span class="nav-text">2.2 logistic sigmoid与双曲正切函数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#其他隐藏层单元"><span class="nav-number">2.3.</span> <span class="nav-text">2.3 其他隐藏层单元</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#架构设计"><span class="nav-number">3.</span> <span class="nav-text">3. 架构设计</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#万能近似"><span class="nav-number">3.1.</span> <span class="nav-text">3.1 万能近似</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#其他架构上的考虑"><span class="nav-number">3.2.</span> <span class="nav-text">3.2 其他架构上的考虑</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#反向传播"><span class="nav-number">4.</span> <span class="nav-text">4. 反向传播</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#微积分中的链式法则"><span class="nav-number">4.1.</span> <span class="nav-text">微积分中的链式法则</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#递归的使用链式法则实现反向传播"><span class="nav-number">4.2.</span> <span class="nav-text">递归的使用链式法则实现反向传播</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#mlp中的的反向传播计算"><span class="nav-number">4.3.</span> <span class="nav-text">MLP中的的反向传播计算</span></a></li></ol></li></ol></div></div></section></div></aside></div></main><footer id="footer" class="footer"><div class="footer-inner"><div class="copyright"> &copy; <span itemprop="copyrightYear">2017</span><span class="with-love"><i class="fa fa-heart"></i></span> <span class="author" itemprop="copyrightHolder">Zhipeng Shen</span></div><div class="powered-by"></div> <span>Hosted by <a href="https://pages.coding.me" style="font-weight:700">Coding Pages</a></span></div></footer><div class="back-to-top"><i class="fa fa-arrow-up"></i></div></div><script type="text/javascript">"[object Function]"!==Object.prototype.toString.call(window.Promise)&&(window.Promise=null)</script><script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script><script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script><script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script><script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script><script type="text/javascript" src="/js/src/utils.js?v=5.1.2"></script><script type="text/javascript" src="/js/src/motion.js?v=5.1.2"></script><script type="text/javascript" src="/js/src/affix.js?v=5.1.2"></script><script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.2"></script><script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.2"></script><script type="text/javascript" src="/js/src/post-details.js?v=5.1.2"></script><script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.2"></script><link rel="stylesheet" href="/lib/algolia-instant-search/instantsearch.min.css"><script src="/lib/algolia-instant-search/instantsearch.min.js"></script><script src="/js/src/algolia-search.js?v=5.1.2"></script><script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.4.js"></script><script>AV.initialize("dEwWhyQR7bLETpAgUiClVWIQ-gzGzoHsz","B035g0HYyXNoqKJcJVVpllir")</script><script>function showTime(e){var t=new AV.Query(e),n=[],o=$(".leancloud_visitors");o.each(function(){n.push($(this).attr("id").trim())}),t.containedIn("url",n),t.find().done(function(e){if(0!==e.length){for(c=0;c<e.length;c++){var t=e[c],i=t.get("url"),s=t.get("time"),l=document.getElementById(i);$(l).find(".leancloud-visitors-count").text(s)}for(var c=0;c<n.length;c++){var i=n[c],l=document.getElementById(i),r=$(l).find(".leancloud-visitors-count");""==r.text()&&r.text(0)}}else o.find(".leancloud-visitors-count").text(0)}).fail(function(e,t){console.log("Error: "+t.code+" "+t.message)})}function addCount(e){var t=$(".leancloud_visitors"),n=t.attr("id").trim(),o=t.attr("data-flag-title").trim(),i=new AV.Query(e);i.equalTo("url",n),i.find({success:function(t){if(t.length>0){var i=t[0];i.fetchWhenSave(!0),i.increment("time"),i.save(null,{success:function(e){$(document.getElementById(n)).find(".leancloud-visitors-count").text(e.get("time"))},error:function(e,t){console.log("Failed to save Visitor num, with error message: "+t.message)}})}else{var s=new e,l=new AV.ACL;l.setPublicReadAccess(!0),l.setPublicWriteAccess(!0),s.setACL(l),s.set("title",o),s.set("url",n),s.set("time",1),s.save(null,{success:function(e){$(document.getElementById(n)).find(".leancloud-visitors-count").text(e.get("time"))},error:function(e,t){console.log("Failed to create")}})}},error:function(e){console.log("Error:"+e.code+" "+e.message)}})}$(function(){var e=AV.Object.extend("Counter");1==$(".leancloud_visitors").length?addCount(e):$(".post-title-link").length>1&&showTime(e)})</script><script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script><script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script><script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script></body></html>