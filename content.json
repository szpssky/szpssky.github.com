{"meta":{"title":"Think Different","subtitle":null,"description":null,"author":"Zhipeng Shen","url":"http://blog.tifosi-m.com"},"pages":[{"title":"","date":"2017-09-29T04:58:45.355Z","updated":"2017-09-29T04:58:45.355Z","comments":true,"path":"archives/index.html","permalink":"http://blog.tifosi-m.com/archives/index.html","excerpt":"","text":"title: All archives date: 2014-12-22 12:39:04 type: “archives” —"},{"title":"分类","date":"2014-12-22T12:39:04.000Z","updated":"2017-09-29T04:58:45.355Z","comments":true,"path":"categories/index.html","permalink":"http://blog.tifosi-m.com/categories/index.html","excerpt":"","text":""},{"title":"标签","date":"2014-12-22T12:39:04.000Z","updated":"2017-09-29T04:58:45.355Z","comments":true,"path":"tags/index.html","permalink":"http://blog.tifosi-m.com/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"《机器学习》笔记 —— EM算法","slug":"ML_EM","date":"2017-09-28T14:00:00.000Z","updated":"2017-09-29T04:58:45.355Z","comments":true,"path":"2017/09/28/ML_EM/","link":"","permalink":"http://blog.tifosi-m.com/2017/09/28/ML_EM/","excerpt":"","text":"EM算法（对含隐变量的模型进行参数估计） 令x表示已观测变量集，z表示隐变量集，\\(\\theta\\)表示模型参数，若对\\(\\theta\\)做极大似然估计，则应最大化对数似然。 \\[ LL(\\theta|x,z)=\\ln P(x,z|\\theta) \\] 由于存在隐变量z，此式无法直接求解，通过对z计算期望，求最大化已观测数据的对数边际似然： \\[ LL(\\theta,x) = \\ln P(x|\\theta)=\\ln \\sum_zP(x,z|\\theta) \\] EM算法基本思想： 若\\(\\theta\\)已知，则可根据训练数据推断最优隐变量z（E步） 若z的值已知，则可根据z对参数\\(\\theta\\)做极大似然估计（M步） 迭代直至收敛： （1）基于\\(\\theta^t\\)推断z的期望记为\\(z^t\\) （2）基于已观测变量x和\\(z^t\\)对\\(\\theta\\)做极大似然估计，记为\\(\\theta^{t+1}\\) 隐变量估计问题也可以通过梯度下降等优化算法求解，但求和项数随隐变量的数目以指数级上升，会给梯度计算带来麻烦，而EM算法为非梯度算法。","categories":[{"name":"Meachine Learning","slug":"Meachine-Learning","permalink":"http://blog.tifosi-m.com/categories/Meachine-Learning/"}],"tags":[{"name":"Meachine Learning","slug":"Meachine-Learning","permalink":"http://blog.tifosi-m.com/tags/Meachine-Learning/"}],"keywords":[{"name":"Meachine Learning","slug":"Meachine-Learning","permalink":"http://blog.tifosi-m.com/categories/Meachine-Learning/"}]},{"title":"《机器学习》笔记 —— 贝叶斯分类器","slug":"ML_Bayes_Classifier","date":"2017-09-22T14:00:00.000Z","updated":"2017-09-29T04:58:45.355Z","comments":true,"path":"2017/09/22/ML_Bayes_Classifier/","link":"","permalink":"http://blog.tifosi-m.com/2017/09/22/ML_Bayes_Classifier/","excerpt":"贝叶斯决策论 假设有N种可能的类别标记，即\\(y=\\{c_1,c_2,\\dots ,c_N\\}\\)，\\(\\lambda_{ij}\\)是将一个真实标记为\\(c_i\\)的标记误分为\\(c_i\\)所产生的损失，基于后验概率\\(p(c_i|x)\\)可获得将样本\\(x\\)分类为\\(c_i\\)所产生的期望损失，即在样本x上的条件风险： \\[ R(c_i|x)=\\sum_{j=1}^N \\lambda_{ij}p(c_j|x) \\] 目标是寻找判定准则h:\\(\\mathcal{X} \\rightarrow \\mathcal{Y}\\)以最小化总体风险。 \\[ R(h) = \\mathbb{E}_x[R(h(x)|x)] \\] 若h最小化条件风险\\(R(h(x)|x)\\)，则总体风险\\(R(x)\\)也将被最小化。","text":"贝叶斯决策论 假设有N种可能的类别标记，即\\(y=\\{c_1,c_2,\\dots ,c_N\\}\\)，\\(\\lambda_{ij}\\)是将一个真实标记为\\(c_i\\)的标记误分为\\(c_i\\)所产生的损失，基于后验概率\\(p(c_i|x)\\)可获得将样本\\(x\\)分类为\\(c_i\\)所产生的期望损失，即在样本x上的条件风险： \\[ R(c_i|x)=\\sum_{j=1}^N \\lambda_{ij}p(c_j|x) \\] 目标是寻找判定准则h:\\(\\mathcal{X} \\rightarrow \\mathcal{Y}\\)以最小化总体风险。 \\[ R(h) = \\mathbb{E}_x[R(h(x)|x)] \\] 若h最小化条件风险\\(R(h(x)|x)\\)，则总体风险\\(R(x)\\)也将被最小化。 贝叶斯判定准则：为最小化总体风险，只需在每个样本上选择那个能使条件风险\\(R(c|x)\\)最小的类别标记，即： \\[ h^*(x)=\\arg\\min_{c \\in \\mathcal{y}}R(c|x) \\] \\(h^*\\)为贝叶斯最优分类器，\\(R(h^*)\\)称为贝叶斯风险，\\(1-R(h^*)\\)反应分类器能达到的最好性能，即精度理论上限。 具体的，若目标是最小化分类错误率，则误判损失\\(\\lambda_{ij}\\)： \\[\\lambda_{ij} = \\left\\{ \\begin{array}{rl} &amp; 0 \\qquad \\text{if }i=j \\\\ &amp; 1 \\qquad \\text{otherwise}\\\\ \\end{array} \\right. \\] 此时条件风险： \\[ R(c|x)=1-p(c|x) \\] 最小化分类错误率的贝叶斯分类器： \\[ h^*(x)=\\arg\\max_{c\\in \\mathcal{y}}p(c|x) \\] 即对每个样本，选择能使其后验概率\\(p(c|x)\\)最大的类别标记。 机器学习实现基于有限的训练样本集尽可能准确地估计出后验概率\\(p(c|x)\\)，两种策略： (1)给定x可通过直接建模\\(p(c|x)\\)来预测c (2)先对联合概率分布\\(p(x,c)\\)建模，再由此获得\\(p(c|x)\\) (1)为判别式模型，(2)为生成式模型 常见判别式模型：逻辑回归、SVM、线性回归、神经网络、条件随机场CRF、线性判别分析LDA、Boosting等。 常见生成式模型：高斯混合模型、隐马尔可夫模型、信念网络、朴素贝叶斯、贝叶斯网络、主题生成模型LDA、马尔可夫随机场等。 对生成模型比如考虑： \\[ P(c|x)=\\frac{P(x,c)}{P(x)} \\] 基于贝叶斯定理： \\[ P(c|x)=\\frac{P(c)P(x|c)}{P(x)} \\] \\(P(c)\\)为类先验概率表达了各类样本所占比例，当训练集包含充足的独立同分布样本时\\(P(c)\\)可通过各类样本出现频率来估计，\\(P(x|c)\\)为样本x相对于类标记c的类条件概率（似然），\\(P(x|c)\\)涉及x所有属性的联合概率，通常存在未被观测到的情况，所以无法通过频率来估计，只能通过“极大似然估计”，\\(P(x)\\)为证据因子，与类标签无关。 极大似然估计 估计类条件概率的一种常见策略是先假定其具有某种确定的概率分布形式，再基于训练样本对概率分布参数进行估计。 令\\(D_c\\)表示训练集D中第c类样本组成的集合，假设这些样本是独立同分布的则参数\\(\\theta_c\\)对于数据集\\(D_c\\)的似然是： \\[ P(D_c|\\theta_c)=\\prod_{x \\in D_c}P(x|\\theta_c) \\] 对\\(\\theta_c\\)进行极大似然估计寻找能最大化似然\\(P(D_c|\\theta_c)\\)的参数值\\(\\hat \\theta_c\\)，连乘操作容易下溢，通常用对数似然： \\[ \\begin{align} LL(\\theta_c) &amp;= \\log P(D_c|\\theta_c) \\\\ &amp;= \\sum_{x \\in D_c}\\log P(x|\\theta_c) \\end{align} \\] 参数\\(\\theta_c\\)的极大似然估计\\(\\hat \\theta_c\\)： \\[ \\hat \\theta_c = \\arg\\max_{\\theta_c} LL(\\theta_c) \\] 注意：这种参数化的方法虽然使类条件概率估计变得相对简单，但估计结果严重依赖于所假设的概率分布形式是否符合真实的数据分布。 朴素贝叶斯分类器 \\(P(x|c)\\)是所有属性上的联合概率，难以从有限样本直接估计（因为在计算上将会遭遇组合爆炸，在数据上将会遭遇样本稀疏且属性越多问题越严重），为了避开这个问题，朴素贝叶斯分类器采用“属性条件独立性假设”（对已知类别，假设所有属性相互独立）。 基于该假设： \\[ \\begin{align} P(c|x)&amp;= \\frac{P(c)P(x|c)}{P(x)} \\\\ &amp;= \\frac{P(c)}{P(x)}\\prod_{i=1}^dP(x_i|c) \\end{align} \\] 其中d表示属性数目，\\(x_i\\)表示x在第i个属性上的取值。由于所有类别\\(P(x)\\)相同，因此朴素贝叶斯表达式为： \\[ h_{nb}(x)=\\arg\\max_{c \\in \\mathcal{Y}} P(c)\\prod_{i=1}^dP(x_i|c) \\] 令\\(D_c\\)表示训练集D中第c类样本组成的集合，若有充分的独立同分布样本，则容易估算出类先验概率\\(P(c)\\)： \\[ P(c)=\\frac{|D_c|}{|D|} \\] 令\\(D_{c，x_i}\\)表示\\(D_c\\)中第i个属性取值为\\(x_i\\)的样本组成的集合，则： \\[ P(x_i|c) = \\frac{|D_{c,x_i}|}{|D_c|} \\] 对连续属性：假设\\(P(x_i|c)\\sim\\mathcal{N}(\\mu_{c,i},\\sigma^2_{c,i})\\)，则： \\[ P(x_i|c)=\\frac{1}{\\sqrt{2\\pi}\\sigma_{c,i}}exp(-\\frac{(x_i-\\mu_{c,i})^2}{2\\sigma^2_{c,i}}) \\] 若某个属性值在训练集中没有与某个类同时出现过，则直接进行概率估计将出现问题，连乘式概率为0，为避免其他属性的信息被“抹去”，需要进行“平滑”。 拉布拉斯修正： 令N表示训练集D中可能的类别数，\\(N_i\\)表示第i个属性可能的取值数则： \\[ \\begin{align} \\hat P(c)&amp;=\\frac{|D_c|+1}{|D|+N} \\\\ \\hat P(x_i|c)&amp;=\\frac{|D_{c,x_i}|+1}{|D_c|+N_i} \\end{align} \\] 半朴素贝叶斯分类器 “属性条件独立性假设”在现实中往往很难成立，对该假设进行一定程度放松则得到“半朴素贝叶斯分类器” “独依赖估计”是半朴素贝叶斯分类器最常用的策略，即假设每个属性在类别之外最多依赖于一个其他属性，即： \\[ P(c|x) \\propto P(c)\\prod_{i=1}^dP(x_i|c,pa_i}) \\] 其中\\(pa_i\\)为\\(x_i\\)所依赖的属性，称为\\(x_i\\)的父属性，若\\(pa_i\\)已知，则可估计概率值\\(P(x_i|c,pa_i)\\)。 1.SPODE：假设所有属性都依赖于同一个属性，称为超父，然后通过交叉验证等方式确定超父属性。 2.TAN：在最大带权生成树的基础上通过以下步骤将属性依赖简约为周志华机器学习P115所示的树形结构。 计算任意两个属性之间的条件互信息。\\(I(x_i,x_j|y)=\\sum_{x_i,x_j;c\\in \\mathcal{Y}}P(x_i,x_j;|c)\\log\\frac{P(x_i,x_j|c)}{P(x_i|c)P(x_j|c)}\\) 以属性为结点构建完全图，任意两个结点之间边的权重为\\(I(x_i,x_j|\\mathcal{Y})\\) 构建此完全图的最大带权生成树，挑选根变量将边置为有向。 TAN仅保留了强相关属性的依赖性 3.AODE：基于集成学习，更强大的独依赖分类器，尝试将每个属性作为超父构建SPODE，然后将具有足够数据支撑的SPOE集成起来： \\[ P(c|x) \\propto \\sum_{\\substack{i=1 \\\\ |D_{x_i}| \\geq m&#39;}}^d \\] 其中\\(D_{x_i}\\)是在第i个属性上的取值为\\(x_i\\)的样本集合，\\(m&#39;\\)为阈值常数，AODE需估计\\(P(c,x_i)\\)和\\(P(x_j|c,x_i)\\)： \\[ \\begin{align} \\hat P(c,x_i)&amp;=\\frac{|D_{c,x_i}|+1}{|D|+N_i} \\\\ \\hat P(x_j|c,x_i) &amp;=\\frac{|D_{c,x_i,x_j}|+1}{|D_c,x_i|+N_j} \\end{align} \\] 若使用高阶依赖，则当训练数据非常充分，泛化性能有可能提升，但在有限样本下则又陷入估计高阶联合概率分布的泥泽。","categories":[{"name":"Meachine Learning","slug":"Meachine-Learning","permalink":"http://blog.tifosi-m.com/categories/Meachine-Learning/"}],"tags":[{"name":"Meachine Learning","slug":"Meachine-Learning","permalink":"http://blog.tifosi-m.com/tags/Meachine-Learning/"}],"keywords":[{"name":"Meachine Learning","slug":"Meachine-Learning","permalink":"http://blog.tifosi-m.com/categories/Meachine-Learning/"}]},{"title":"《机器学习》笔记 —— 集成学习","slug":"ML_Ensemble_Learning","date":"2017-09-15T09:00:00.000Z","updated":"2017-09-29T04:58:45.355Z","comments":true,"path":"2017/09/15/ML_Ensemble_Learning/","link":"","permalink":"http://blog.tifosi-m.com/2017/09/15/ML_Ensemble_Learning/","excerpt":"通过构建并结合多个学习器来完成学习任务。 个体学习器通常由一个现有学习算法从训练数据产生，再由某种策略将他们结合起来。 同质集成中的个体学习器亦称为“基学习器”，相应的学习算法称为“基学习算法”。 异质集成中的个体学习器由不同学习算法组成，个体学习器称为“组件学习器” 考虑二分类问题\\(y\\in \\{-1,+1\\}\\)和真实函数\\(f\\),假定基学习器的错误率为\\(\\epsilon\\)，即对每个即分类器\\(h_i\\)有： \\[ p(h_i(x)\\neq f(x)) = \\epsilon \\] 用简单投票法结合T个基分类器，若超过半数的基分类器正确，则分类正确： \\[ H(x)=sign(\\sum_{i=1}^Th_i(x)) \\]","text":"通过构建并结合多个学习器来完成学习任务。 个体学习器通常由一个现有学习算法从训练数据产生，再由某种策略将他们结合起来。 同质集成中的个体学习器亦称为“基学习器”，相应的学习算法称为“基学习算法”。 异质集成中的个体学习器由不同学习算法组成，个体学习器称为“组件学习器” 考虑二分类问题\\(y\\in \\{-1,+1\\}\\)和真实函数\\(f\\),假定基学习器的错误率为\\(\\epsilon\\)，即对每个即分类器\\(h_i\\)有： \\[ p(h_i(x)\\neq f(x)) = \\epsilon \\] 用简单投票法结合T个基分类器，若超过半数的基分类器正确，则分类正确： \\[ H(x)=sign(\\sum_{i=1}^Th_i(x)) \\] 假设基分类器错误率相互独立，则有Hoeffding不等式可得集成错误率为： \\[ \\begin{align} p(H(x)\\neq f(x)) &amp;=\\sum_{k=0}^{\\lfloor T/2 \\rfloor}C_T^k(1-\\epsilon)^k\\epsilon^{T-k} \\\\ &amp; \\leq exp(-\\frac{1}{2}T(1-2\\epsilon)^2) \\end{align} \\] 随着T增大，错误率将以指数级下降。 根据个体学习器的生成方式，目前集成学习方法大致可以分成两类，一类是个体学习器之间存在强依赖关系、必须串行生成的序列化方法，代表是Boosting方法，第二类是个体学习器之间不存在强依赖关系、可同时生成的并行方法，代表是Bagging和随机森林。 Boosting Boosting是一族可将弱学习器提示为强学习器的算法，这族算法的工作机制：先从初始训练集训练出一个基学习器，再根据基学习器的表现对训练样本分布进行调整，使得先前基学习器做错的样本在后续受到更多关注，然后基于调整后的样本分布来训练下一个基学习器，如此重复知道基学习器达到事先指定的值T，最终将这T个基学习器进行加权结合。 AdaBoosting算法： 12345678910111213输入：训练集D=&#123;(x1,y1),(x2,y2),...,(xm,ym)&#125; 基学习器算法L 训练轮数T过程： D1(x) = 1/m for t = 1,2,...,T do ht = L(D,Dt) et = P(ht(x)!=f(x)) if et &gt; 0.5 then break alpha_a = 1/2ln((1-et)/et) Dt+1(x) = Dt(x)/Zt x exp(-alpha_t*f(x)ht(x)) end for输出：H(x) = sign(\\sum_t^T alpha_tht(x)) Boosting算法要求基学习器能对特定的数据分布进行学习，这可通过“重赋权法”实施，即在训练过程的每一轮中，根据样本分布为每个训练样本重新赋予一个权重，对无法接受带权样本的基学习算法，则可通过“重采样法”来处理，即在每一轮中，根据样本分布对训练集重新进行采样，再用重采样而得的样本集对基学习器进行训练。Boosting算法在训练的每一轮都要检查当前生成的基学习器是否满足基本条件（检查当前基分类器是否比随机猜想好），一旦条件不满足，则当前基学习器被抛弃且学习过程停止。在此情况下，初始设置的轮数T还远远没达到，科恩个导致最终因基学习器数量较少而性能不佳，若采用“重采样法”则可重启动的训练，避免训练过早停止，即抛弃不满足条件的基学习器后，根据当前分布对训练样本进行重新采样，再基于新的采样结果重新训练。 从偏差-方差角度看，Boosting主要关注降低偏差，因此Boosting能基于泛化性能相对弱的学习器构建很强的集成。 Bagging 并行式集成学习方法的代表，基于自助采样，采样出T个含m个训练样本的采样，然后基于每个采样集训练出一个基学习器，再将基学习器进行结合。Bagging通常对分类任务使用简单投票法，对回归任务使用简单平均法。 训练一个Bagging集成与直接使用基学习算法训练一个学习器复杂度同阶，由于每个基学习器只使用了初始训练集中约63.2%的样本，剩下约36.8%的样本可用作验证集对泛化性能进行包外估计。 当基学习器是决策的时，可使用包外样本来辅助剪枝或用于估计决策树中各结点的后验概率以辅助队零训练样本结点的处理。当基学习器是神经网络时，可用来辅助提取终止减小过拟合。 从偏差-方差角度，Bagging主要关注降低方差，因此其在不剪枝决策树、神经网络等易受样本扰动的学习器上效果更为明显。 随机森林 随机森林是Bagging的一个扩展变体，RF在以决策树为基学习器构建Bagging集成的基础上，在决策树训练过程中引入随机属性。 对基决策树的每个结点，先从该结点的属性集合中随机选择一个包含k个属性的子集，然后再从这个子集中选择一个最优属性用于划分，k控制随机性引入程度，一般情况下推荐值\\(k=\\log_2d\\)，随机森林的训练效率常优于Bagging，因为在个体决策树构建过程中，Bagging使用确定型决策树，而RF使用“随机型”决策树（只需考虑一个属性子集）。 结合策略 平均法 1）简单平均法： \\[ H(x)=\\frac{1}{T}\\sum_{i=1}^Th_i(x) \\] 加权平均法： \\[ H(x)=\\sum_{i=1}^Tw_ih_i(x) \\] 其中\\(w_i\\)是个体学习器\\(h_i\\)的权重，通常要求\\(w_i \\geq 0 \\text{, } \\sum_{i=1}^Tw_i=1\\) 加权平均法的权重一般是从训练数据中学习而得，现实中训练样本通常不充分或存在噪声，这使得学得的权重并不可靠，尤其是对规模比较大的集成来说，要学习的权重比较多，较容易出现过拟合，因此加权平均有时候未必优于简单平均法。 投票法 假设学习器\\(h_i\\)在样本x的预测输出表示为一个N维向量\\((h_i^1(x);h_i^2(x);\\dots ; h_i^N(x))\\)，其中\\(h_i^j(x)\\)是\\(h_i\\)在类别标记\\(c_j\\)上的输出。 1）绝对多数投票法： \\[H(x) = \\left\\{ \\begin{array}{rl} &amp; c_j &amp; \\qquad \\quad \\text{if }\\sum_{i=1}^Th_i^j(x)&gt;0.5 \\sum_{k=1}^N\\sum_{i=1}^Th_i^k(x) \\\\ &amp; \\text{reject}&amp; \\qquad \\quad \\text{otherwise}\\\\ \\end{array} \\right. \\] 若某标记得投票超过半数，则预测为该标记，否则拒绝预测。 2）相对多数投票法 \\[ H(x)=c_{\\arg\\max_j \\sum_{i=1}^Th_i^j(x)} \\] 即预测为得票最多的标记，若同时有多个标记获得最高票，则从中随机选取一个 3）加权投票法 \\[ H(x)=c_{\\arg\\max_j\\sum_{i=1}^Tw_ih_i^j(x)} \\] 与加权平均法类似，\\(w_i\\)是\\(h_i\\)的权重，通常\\(w_i \\geq 0\\)，\\(\\sum_{i=1}^Tw_i=1\\)","categories":[{"name":"Meachine Learning","slug":"Meachine-Learning","permalink":"http://blog.tifosi-m.com/categories/Meachine-Learning/"}],"tags":[{"name":"Meachine Learning","slug":"Meachine-Learning","permalink":"http://blog.tifosi-m.com/tags/Meachine-Learning/"}],"keywords":[{"name":"Meachine Learning","slug":"Meachine-Learning","permalink":"http://blog.tifosi-m.com/categories/Meachine-Learning/"}]},{"title":"《Deep Learning》笔记 —— 卷积神经网络","slug":"Convolutional_Networks","date":"2017-09-01T20:00:00.000Z","updated":"2017-09-29T04:58:45.351Z","comments":true,"path":"2017/09/01/Convolutional_Networks/","link":"","permalink":"http://blog.tifosi-m.com/2017/09/01/Convolutional_Networks/","excerpt":"卷积网络是一种专门用来处理具有类似网格结构的数据的神经网络，例如时间序列数据(可以认为是在时间轴上有规律地采样形成的一维网格)和图像数据(二维像素网格)，卷积网络在诸多应用领域都表现优异。卷积网络是指那些至少网络的一层中使用卷积运算来代替一般乘法运算的神经网络。 卷积运算 在通常形式中，卷积是对两个实变函数的一种数学运算。卷积运算通常用星号表示： \\[ s(t)=(x*w)(t) \\] 卷积的第一个参数通常叫作输入，第二个参数叫作核函数，输出有时被称作特征映射。 在机器学习应用中，输入通常是多维数组的数据，而核通常是由学习算法优化得到的多维数组参数。如果把一张二维图像I作为输入，也使用一个二维的核K： \\[ S(i,j)=(I*K)(i,j)=\\sum_m\\sum_n I(m,n)K(i-m,j-n) \\]","text":"卷积网络是一种专门用来处理具有类似网格结构的数据的神经网络，例如时间序列数据(可以认为是在时间轴上有规律地采样形成的一维网格)和图像数据(二维像素网格)，卷积网络在诸多应用领域都表现优异。卷积网络是指那些至少网络的一层中使用卷积运算来代替一般乘法运算的神经网络。 卷积运算 在通常形式中，卷积是对两个实变函数的一种数学运算。卷积运算通常用星号表示： \\[ s(t)=(x*w)(t) \\] 卷积的第一个参数通常叫作输入，第二个参数叫作核函数，输出有时被称作特征映射。 在机器学习应用中，输入通常是多维数组的数据，而核通常是由学习算法优化得到的多维数组参数。如果把一张二维图像I作为输入，也使用一个二维的核K： \\[ S(i,j)=(I*K)(i,j)=\\sum_m\\sum_n I(m,n)K(i-m,j-n) \\] 卷积是可交换的，所以上式等价于： \\[ S(i,j)=(K*I)(i,j)=\\sum_m\\sum_n I(i-m,j-n)K(m,n) \\] 卷积运算可交换性是将核相对输入进行了翻转，将核翻转的唯一目的是实现可交换性，尽管可交换性在证明时有用，但在神经网络的应用中却不是一个重要的性质。 动机 卷积运算通过三个重要的思想来帮助改进机器学习系统：稀疏交互、参数共享、等变表示。 传统神经网络使用矩阵乘法来建立输入与输出的连接关系，其中，参数矩阵中每个单独的参数都描述了一个输入单元与一个输出单元间的交互，然后卷积网络具有稀疏交互(稀疏连接)，通过使核大小远小于输入的大小来达到这个特点。当处理一个图像时通过只使用几十到几百个像素点的核来检测一些小的有意义的特征，例如图像的边缘。尽管神经单元的直接连接都是稀疏的，但是在深度卷网络中，处在深层的单元间接地连接到全部或大部分的输入。 在卷积网络中，核的每个元素都作用在输入的每一个位置上，卷积运算中的参数共享保证了只需要学习一个参数集合，而不是对于每一个位置都需要学习一个单独的参数集合，因此卷积在存储和统计效率方面极大地优于稠密矩阵的乘法运算。 对于卷积，参数共享的特殊形式使得神经网络层具有平移等变的性质，如果一个函数满足输入改变，输出也以同样的方式改变着一个性质，则称这是等变的。 池化 池化函数使某一位置相邻输出的总体统计特征来代替网络在该位置的输出，例如最大池化函数给出相邻矩形区域内的最大值，其他常用的池化函数包括相邻区域内的平均值、\\(L^2\\)范数以及基于距中心像素距离的加权平均函数。 当输入做出少量平移时，经过池化函数后的大多数输出并不会发生改变。局部平移不变性是一个非常有用性质，尤其是当我们只需要关心某个特征是否出现而不需要关心它出现的具体位置时。 使用池化可以看做增加了一个无限强的先验：这一层学得的函数必须具有对少量平移的不变性。当这个假设成立时，池化可以极大提高网络的统计效率。 卷积与池化作为一种无限强的先验 卷积的使用相当于对网络中的一层的参数引入一个无限强的概率分布先验证：一个隐藏层单元的权重必须和它的邻居的权重相同，但可以在空间上移动，即该先验说明该层应该学得的函数只包含局部连接关系并且对平移具有等变性。 池化也是一个无限强的先验：每一个单元都具有少量平移不变性。 卷积和池化可能导致欠拟合，卷积和池化只有当先验的假设合理且正确时才有用，如果一项任务依赖于保存精确的空间信息，那么在所有特征上使用池化将增大训练误差。 基本卷积函数的变体 非共享卷积： 在一些情况下，并不是真的想使用卷积，而是想用一些局部连接的网络层，在这种情况下多层感知机对应的邻接矩阵是相同，但每一个连接都有它自己的权重。因为它和具有一个小核的离散卷积运算很像，但并不跨位置来共享参数。 平铺卷积： 对卷积层和局部连接层进行折中，这里并不是对每一个空间位置的权重集合进行学习，而是学习一组核使得在空间移动时它们可以被循环利用，这意味着在近邻的位置上拥有不同的滤波器，就像局部连接层一样，但是对于参数的存储需求仅仅会增长常数倍，如下图对局部连接、平铺卷积和标准卷积进行比较： 现在仍不清楚为什么卷积网络在一般的反向传播网络被认为已经失败时反而成功了，这可能可以简单的归结为卷积网络比全连接网络计算效率更高，因此使用它们运行多个实验并调整它们的实现和超参数更容易，更大的网络也似乎更容易训练。 卷积网络提供了一种方法来特化神经网络，使其能够处理具有清楚的网格结构拓扑的数据，以及将这样的模型扩展到非常大的规模，这种方法在二维图像拓扑上是最成功的。","categories":[{"name":"Deep Learning Book","slug":"Deep-Learning-Book","permalink":"http://blog.tifosi-m.com/categories/Deep-Learning-Book/"}],"tags":[{"name":"Deep Learning","slug":"Deep-Learning","permalink":"http://blog.tifosi-m.com/tags/Deep-Learning/"}],"keywords":[{"name":"Deep Learning Book","slug":"Deep-Learning-Book","permalink":"http://blog.tifosi-m.com/categories/Deep-Learning-Book/"}]},{"title":"《机器学习》笔记 —— 决策树","slug":"ML_Decision_Tree","date":"2017-08-31T19:00:00.000Z","updated":"2017-09-29T04:58:45.355Z","comments":true,"path":"2017/08/31/ML_Decision_Tree/","link":"","permalink":"http://blog.tifosi-m.com/2017/08/31/ML_Decision_Tree/","excerpt":"基本流程 一般的，一棵决策树包含一个根结点，若干个内部结点和若干个叶结点，叶结点对应于决策结果，其他每个结点对应于一个属性测试，每个结点包含的样本集合根据属性测试的结果被划分到子结点中，从根结点到叶结点的路径对应了一个判断测试序列。 决策树学习基本算法: 1234567891011121314151617181920输入：训练集D=&#123;(x1,y1),(x2,y2),...,(xm,ym)&#125; 属性集合A=&#123;a1,a2,...,ad&#125;过程： TreeGenerate(D,A) 生成结点node if D中样本全属于同一类C then 将node结点标记为C类叶结点；return end if if A=null或D中样本在A上取值相同 then 将node标记为叶结点，其类别标记为D种样本最多的类；return end if 从A中选择最优划分属性a* for a* 中每一个值a*^ do 为node生成一个分支:令D_v表示D中在a*上取值为a*^的样本子集 if D_v=null then 将分支标记为叶结点，其类别为D中样本最多的类；return else 以TreeGenerate(D_v,A\\&#123;a*&#125;)为分支结点 end if end","text":"基本流程 一般的，一棵决策树包含一个根结点，若干个内部结点和若干个叶结点，叶结点对应于决策结果，其他每个结点对应于一个属性测试，每个结点包含的样本集合根据属性测试的结果被划分到子结点中，从根结点到叶结点的路径对应了一个判断测试序列。 决策树学习基本算法: 1234567891011121314151617181920输入：训练集D=&#123;(x1,y1),(x2,y2),...,(xm,ym)&#125; 属性集合A=&#123;a1,a2,...,ad&#125;过程： TreeGenerate(D,A) 生成结点node if D中样本全属于同一类C then 将node结点标记为C类叶结点；return end if if A=null或D中样本在A上取值相同 then 将node标记为叶结点，其类别标记为D种样本最多的类；return end if 从A中选择最优划分属性a* for a* 中每一个值a*^ do 为node生成一个分支:令D_v表示D中在a*上取值为a*^的样本子集 if D_v=null then 将分支标记为叶结点，其类别为D中样本最多的类；return else 以TreeGenerate(D_v,A\\&#123;a*&#125;)为分支结点 end if end 划分选择 信息熵增益 信息熵：度量样本集合纯度最常用的一种指标，假定样本集D中第k类样本所占比例为\\(p_k(k=1,2,\\dots,|y|)\\)，则D的信息熵： \\[ Ent(D)=-\\sum_{k=1}^{|y|}p_k\\log_2p_k \\] Ent(D)越小，D的纯度越高。 假定离散数学a有v个可能的取值\\({a^1,a^2,\\dots ,a^v}\\)，若使用a来对样本集D划分，则会产生v个结点，第v个结点包含样本D中所有在属性a上取值为\\(a_v\\)的样本记为\\(D^v\\)，根据不同分支包含的样本数不同，给分支结点赋权重\\(\\frac{|D^v|}{|D|}\\)，则信息熵增益为： \\[ Gain(D,a)=Ent(D)-\\sum_{i=1}^v\\frac{|D^v|}{|D|}Ent(D^v) \\] Gain(D,a)越大，以a划分，纯度提升越大，故划分选择： \\(a_*=\\arg\\max_{a\\in A}Gain(D,a)\\) 增益率 信息熵增准则对可取数目较多的属性有所偏好，为减少这种偏好可能的不利影响，C4.5算法使用增益率来选择最优划分属性： \\[ Gain\\_radio(D,a)=\\frac{Gain(D,a)}{IV(a)} \\] 其中\\(IV(a)=-\\sum\\limits_{v=1}^V\\frac{|D^v|}{|D|}\\log_2 \\frac{|D^v|}{D}\\),称为属性a的固有值，属性a的取值数目越多(v越大)，则IV(a)越大。 增益率对数目较少的属性有所偏好，因此C4.5并不直接选取增益率最大的，而是使用一个启发式方法，先从候选划分属性中找出信息熵高于平均水平的属性，再选增益率最高的。 基尼指数 CART决策树使用“基尼指数”： \\[ \\begin{align} Gini(D) &amp; = \\sum_{k=1}^{|y|}\\sum_{k&#39;\\neq k}p_kp_{k&#39;} \\\\ &amp; = 1-\\sum_{k=1}^{|y|}p_k^2 \\end{align} \\] 则对属性a， \\[ \\text{Gini_index}(D,a)=\\sum_{i=1}^|v|\\frac{|D^v|}{|D|}Gini(D^v) \\\\ a_* = \\arg\\min_{a \\in A}\\text{Gini_index}(D,a) \\] 剪枝处理 决策树对方“过拟合”的主要手段。 预剪枝 决策树生成过程中对每个结点在划分前先进行估计，若当前结点的划分不能带来决策树泛化性能提升，则停止划分并将当前结点标记为叶结点。 预剪枝使得决策树的很多分支都没有展开，这不仅降低了过拟合的风险，还减少了训练期间开销和测试时间，但有些分支的当前划分虽不能提升泛化性能，但在其基础上进行后续划分却可能提升泛化性能，其基于“贪心”本质禁止分支展开，故有欠拟合风险。 后剪枝 先生成一颗完整决策树，再自底向上对非叶结点进行考察，若将该结点对应的子树替换为叶结点能提升性能则进行替换。 优点：欠拟合风险很小，泛化性能优于预剪枝决策树 缺点：训练时间开销比未剪枝和预剪枝决策树都要大很多 连续值处理(连续属性离散化) C4.5决策树处理策略：二分法 对样本集D和连续属性a，假的a有n个取值，将这些值进行排序，记为\\({a^1,a^2,\\cdots,a^n}\\)，基于划分点\\(t\\)可将D划分为\\(D_t^-\\)和\\(D_t^+\\)，对相邻属性取值\\(a^i\\)和\\(a^{i+1}\\)，t在\\([a^i,a^{i+1})\\)中任意值所产生的划分结果相同。 考察包含n-1个元素的候选划分点集合： \\[ T_a=\\bigg \\{\\frac{a^i+a^{i+1}}{2}\\bigg| 1 \\leq i \\geq n-1 \\bigg\\} \\] 即， \\[ \\begin{align} \\text{Gain}(D,a) &amp; =\\max_{t\\in T_a}\\text{Gain}(D,a,t) \\\\ &amp; = \\max_{t \\in T_a}\\text{Ent}(D)-\\sum_{\\lambda \\in \\{-,+\\}}\\frac{|D_t^\\lambda|}{|D|}\\text{Ent}(D_t^\\lambda) \\end{align} \\] 缺少值处理 需要解决两个问题： 如何在属性缺失的情况下进行划分属性选择 给定训练集D合属性a，令\\(\\tilde{D}\\)表示D中在a上没有缺失值的子集，假设属性a有\\(v\\)个可能取值\\(\\{a^1,a^2,\\cdots,a^v \\}\\)，令\\(\\tilde{D}^v\\)表示\\(\\tilde{D}\\)中在属性a上取值为\\(a^v\\)的样本子集，\\(\\tilde{D}_k\\)表示\\(\\tilde{D}\\)中属于第k类\\((k=1,2,\\cdots,|y|)\\)的样本子集，则\\(\\tilde{D}=U_{k=1}^{|y|}\\tilde{D}_k\\)，\\(\\tilde{D}_{v=1}^V\\tilde{D}^v\\)，假定对每个样本赋权重\\(w_x\\)， 定义： \\[ \\rho = \\frac{\\sum_{x\\in \\tilde{D}}w_x}{\\sum_{x\\in D}w_x} \\\\ \\tilde{p}_k = \\frac{\\sum_{x \\in \\tilde{D}_k}w_x}{\\sum_{x \\in \\tilde{D}}w_x} \\\\ \\tilde{\\gamma}_v = \\frac{\\sum_{x\\in \\tilde{D}}w_x}{\\sum_{x \\in \\tilde{D}}w_x} \\] \\(\\rho\\)表示无缺失值样本所占比例 \\(\\tilde{p}_k\\)表示无缺失值样本中第k类所占比例 \\(\\tilde{\\gamma}_v\\)表示无缺失值样本中在属性\\(a\\)上取值\\(a^v\\)的样本所占比例 则，\\(\\sum_{k=1}^{|y|}\\tilde{p}_k=1\\)，\\(\\sum_{v=1}^v \\tilde{\\gamma}_v=1\\) 将信息熵增益推广： \\[ \\begin{align} text{Gain}(D,a) &amp; =\\rho \\times \\text{Gain}(\\tilde{D},a) \\\\ &amp; = \\rho \\times (\\text{Ent}(\\tilde{D})-\\sum_{v=1}^V\\tilde{\\gamma}_v \\text{Ent}(\\tilde{D}^v)) \\end{align} \\] 其中 \\[ \\text{Ent}(\\tilde{D})=-\\sum_{k=1}^{|y|}\\tilde{p}_k\\log_2 \\tilde{p}_k \\] 给定划分属性，若样本在该属性缺失，如何对属性进行划分 若样本\\(x\\)在划分属性\\(a\\)上的取值已知，则将\\(x\\)划分入与其取值对应子结点，且样本权值在子结点中保持为\\(w_x\\)，若样本\\(x\\)在划分属性\\(a\\)上取值未知，则\\(x\\)同时划入所有子结点且样本权值在与属性\\(a^v\\)对应的子结点中调整为\\(\\tilde{r}_vw_x\\)，直观上让同一样本以不同概率划入不同子结点中。","categories":[{"name":"Meachine Learning","slug":"Meachine-Learning","permalink":"http://blog.tifosi-m.com/categories/Meachine-Learning/"}],"tags":[{"name":"Meachine Learning","slug":"Meachine-Learning","permalink":"http://blog.tifosi-m.com/tags/Meachine-Learning/"}],"keywords":[{"name":"Meachine Learning","slug":"Meachine-Learning","permalink":"http://blog.tifosi-m.com/categories/Meachine-Learning/"}]},{"title":"《Deep Learning》笔记 —— 深度学习中的正则化","slug":"Regularization_for_Deep_Learning","date":"2017-08-27T08:00:00.000Z","updated":"2017-09-29T04:58:45.355Z","comments":true,"path":"2017/08/27/Regularization_for_Deep_Learning/","link":"","permalink":"http://blog.tifosi-m.com/2017/08/27/Regularization_for_Deep_Learning/","excerpt":"参数范数惩罚 许多正则化的方法通过对目标函数\\(J\\)添加一个参数范数惩罚\\(\\Omega(\\theta)\\)，限制模型(如神经网络、线性回归或逻辑回归)的学习能力。我们将正则化后的目标记为\\(\\tilde{J}\\)： \\[ \\tilde{J}(\\theta;X,y)=J(\\theta;X,y)+\\alpha\\Omega(\\theta) \\] 其中\\(\\alpha\\in [0,\\infty]\\)是权衡惩罚项\\(\\Omega\\)和目标函数\\(\\tilde{J}(X,\\theta)\\)相对贡献的超参数。\\(\\alpha\\)为0则没有正则化，\\(\\alpha\\)越大，对应正则化惩罚越大。 在神经网络中，参数包括每一层仿射变换的权重和偏置，通常只对权重做惩罚，而不对偏置做正则惩罚，每个偏置只控制一个单变量，即使不对其进行正则化也不会导致太大方差，此外正则化偏置参数可能导致欠拟合。 \\(L^2\\)参数正则化 这个正则化的策略是向目标函数添加一个正则项\\(\\Omega(\\theta)=\\frac{1}{2}||w||_2^2\\)，使权重更接近原点。","text":"参数范数惩罚 许多正则化的方法通过对目标函数\\(J\\)添加一个参数范数惩罚\\(\\Omega(\\theta)\\)，限制模型(如神经网络、线性回归或逻辑回归)的学习能力。我们将正则化后的目标记为\\(\\tilde{J}\\)： \\[ \\tilde{J}(\\theta;X,y)=J(\\theta;X,y)+\\alpha\\Omega(\\theta) \\] 其中\\(\\alpha\\in [0,\\infty]\\)是权衡惩罚项\\(\\Omega\\)和目标函数\\(\\tilde{J}(X,\\theta)\\)相对贡献的超参数。\\(\\alpha\\)为0则没有正则化，\\(\\alpha\\)越大，对应正则化惩罚越大。 在神经网络中，参数包括每一层仿射变换的权重和偏置，通常只对权重做惩罚，而不对偏置做正则惩罚，每个偏置只控制一个单变量，即使不对其进行正则化也不会导致太大方差，此外正则化偏置参数可能导致欠拟合。 \\(L^2\\)参数正则化 这个正则化的策略是向目标函数添加一个正则项\\(\\Omega(\\theta)=\\frac{1}{2}||w||_2^2\\)，使权重更接近原点。 添加正则化项后一个模型具有以下目标函数： \\[ \\tilde{J}(w;X,y)=\\frac{\\alpha}{2}w^Tw+J(w;X,y) \\] 与之对应的梯度为： \\[ \\nabla_w\\tilde{J}(w;X,y)=\\alpha w+\\nabla_wJ(w;X,y) \\] 使用单步梯度下降更新权重： \\[ w \\leftarrow w-\\epsilon(\\alpha w+\\nabla_wJ(w;X,y)) \\] 即： \\[ w \\leftarrow (1-\\epsilon \\alpha)w-\\epsilon \\nabla_wJ(w;X,y) \\] 每步执行通常的梯度更新前先收缩权重向量。 令\\(w^*\\)为未正则化的目标函数取得最小误差时权重，即\\(w^*=\\arg\\min_wJ(w)\\)，在\\(w^*\\)邻域做二次近似则近似的\\(\\hat J(\\theta)\\)： \\[ \\hat J(\\theta)=J(w^*)+\\frac{1}{2}(w-w^*)^TH(w-W^*) \\] 其中\\(H\\)是\\(J\\)在\\(w^*\\)处计算的Hessian矩阵，因为在\\(w^*\\)处取得最优，所以一阶导为0，所以二次近似项中没有一次项，又因\\(w^*\\)是J的一个最优点，所以\\(H\\)是半正定的。 当\\(\\tilde{J}\\)取得最小时，其梯度为0： \\[ \\nabla_w\\tilde{J}(w)=H(w-w^*) \\] 对上式添加权重衰减梯度，最小化正则化后\\(\\hat J\\)，\\(\\tilde{w}\\)表示此时的最优点： \\[ \\alpha\\tilde{w}+H(\\tilde{w}-w^*) =0 \\\\ (H+\\alpha I)\\tilde{w}=Hw^* \\\\ \\tilde{w}=(H+\\alpha I)^{-1}Hw^* \\] 因为\\(H\\)是实对称的，将其分解为一个对角矩阵\\(\\Lambda\\)和一组特征向量的标准正交基\\(Q\\)，并且\\(H=Q\\Lambda Q^T\\)： \\[ \\begin{aligned} \\tilde{w} &amp; =(Q\\Lambda Q^T+\\alpha I)^{-1}Q\\Lambda Q^Tw^* \\\\ &amp; =[Q(\\Lambda+\\alpha I)Q^T]^{-1}Q\\Lambda Q^T w^* \\\\ &amp; =Q(\\Lambda + \\alpha I)^{-1}\\Lambda Q^Tw^* \\end{aligned} \\] 权重衰减沿着\\(H\\)特征向量所定义的轴缩放\\(w^*\\)，具体会根据\\(\\frac{\\lambda_i}{\\lambda_i+\\alpha}\\)因子缩放与\\(H\\)第\\(i\\)个特征向量对齐的\\(w^*\\)分量。 沿着\\(H\\)特征值较大的方向正则化的影响较小，而\\(\\lambda_i \\ll \\alpha\\)的分量会搜索到几乎为0。 只有在显著减小目标函数的方向上的参数会保留相对完好，在对目标函数减小的方向没有帮助的方向上的权重会因正则化而被衰减掉。 \\(L^1\\)正则化 \\(L^1\\)正则化被定义为： \\[ \\Omega(\\theta)=||w||_1=\\sum_i|w_i| \\] 即各个参数的绝对值之和，正则化的目标函数\\(\\tilde{J}(w;X,y)\\)： \\[ \\tilde{J}(w;X,y)=\\alpha ||w||_1+J(w;X,y) \\] 对应的梯度为： \\[ \\nabla_w \\tilde{J}(w;X,y)=\\alpha sign(w)+\\nabla_wJ(w;X,y) \\] 正则化对梯度的影响不在是线性缩放每个\\(w_i\\)，而是添加一项与\\(sign(w)\\)同号的常数。 同样的假定模型为简单的线性模型具有二次代价函数，可以通过泰勒级数表示则： \\[ \\nabla_w\\hat J(w)=H(w-w^*) \\] \\(L^1\\)在完全一般化的Hessian的情况下，无法得到清晰的代数表达式，进一步假设Hessian是对角的即\\(H=diag([H_{1,1},\\dots,H_{n,n}])\\),其中\\(H_{i,i} &gt; 0\\)。 将\\(L^1\\)正则化目标函数的二次近似分解成关于参数的求和： \\[ \\hat J(w;X,y)=J(w^*;X,y)+\\sum_i\\big[\\frac{1}{2}H_{i,i}(w_i,w_i^*)^2+\\alpha|w_i|\\big] \\] 下列这个解析解可以最小化上诉近似代价函数： \\[ w_i=sign(w_i^*)\\max\\big\\{|w_i^*|-\\frac{\\alpha}{H_{i,i}},0\\big\\} \\] 对每一个\\(i\\)，\\(w_i^* &gt; 0\\)会有两种结果： 1）\\(w_i^* \\leq \\frac{\\alpha}{H_{i,i}}\\)，正则化后目标\\(w_i\\)最优值是\\(w_i=0\\)，在方向i上\\(J(w;X,y)\\)对\\(\\hat j(w;X,y)\\)的贡献被抵消，\\(L^1\\)正则将\\(w_i\\)推向0。 2）\\(w_i^* &gt; \\frac{\\alpha}{H_{i,i}}\\)，正则化不会将\\(w_i\\)的最优值推向0，而仅仅在这个方向上移动\\(\\frac{\\alpha}{H_{i,i}}\\)。 \\(w_i^* &lt; 0\\)的情况与之类似。 相比\\(L^2\\)正则化，\\(L^1\\)正则化将会产生更稀疏的解。 由\\(L^1\\)正则导出的稀疏性质广泛的应用于特征选择机制，可从可用的特征子集中选择出有意义的特征，简化机器学习问题。 数据集增强 让机器学习模型泛化更好的最好的办法是使用更多的数据进行训练，针对数据集有限的情况，解决这个问题的一种方法是创建假数据并添加到训练集中。在对象识别中，如沿图像每个方向平移几个像素，旋转图像或缩放图像等。在神经网络中注入噪声也可以看做数据增强的一种方式。 噪声鲁棒性 对于某些模型而言相输入添加方差极小的噪声等价于对权重施加范数惩罚，在一般情况下，注入噪声远比简单地收缩参数强，特别是噪声被添加进隐藏单元，最小化带权重噪声的\\(J\\)等同于最小化附带正则项，这种形式的正则化鼓励参数进入权重小扰动对输出相对影响较小的参数空间区域。 还有一种方法是向输出目标注入噪声，大多数数据集的y标签都有一定错误，错误的y不利于最大化\\(\\log p(y|x)\\)，避免这种情况的一种方法是显示地对标签上噪声进行建模，例如可以假设小常数\\(\\epsilon\\)，训练集标记\\(y\\)是正确的概率是\\(1-\\epsilon\\)，任何其他可能的标签也可能是正确的，这个假设很容易与代价函数结合，而不需要显式抽取噪声样本，例如标签平滑通过把确切的分类目标从0和1替换为\\(\\frac{\\epsilon}{k-1}\\)和\\(1-\\epsilon\\)，正则化\\(k\\)个输出的softmax函数模型。 通常使用softmax函数和明确目标的最大似然学习可能永远不会收敛，因为softmax函数永远无法正真预测0概率和1概率，因此可能会继续学习到越来越大的权重，是预测更极端，使用权重衰减正则化可以防止这种情况，但标签平滑的优势是能够防止追求确切的概率，而不阻碍学习正确分类。 多任务学习 多任务学习通过合并几个任务中的样例来提高泛化，可以视为对参数施加软约束。 提取终止 通常会观察到训练误差会随着施加的推移逐渐降低但验证误差会再次上升，所以提前终止策略也是有效降低泛化误差的一种手段，这也是深度学习中最常用的正则化形式。 Bagging和其他集成方法 Bagging是通过结合几个模型降低泛化误差的技术，主要想法是分别训练几个不同的模型，然后让所有模型表决测试样例的输出，被称为模型平均。 神经网络即使所有模型都在同一数据集上训练，也能够找到足够多的不同解，神经网络中随机初始化的差异、小批量的随机选择、超参数或不同输出的非确定性实现往往足以使的集成中的不同成员具有独立的误差。 Dropout Dropout提供了正则化一大类模型的方法，可被认为是集成大量深层神经网络的实用Bagging方法。 Dropout训练集成包括从基础网络中除去非输出单元后形成的子网络，其目标是在指数级数量的神经网络上近似Bagging过程，在训练中使用Dropout，在每次小批量中加载一个样本，然后随机抽样应用于网络中所有输入和隐藏单元的不同二值掩码。 Dropout和Bagging训练的一个显著区别是，在Bagging下所有模型都是独立的，并且每个模型都将在其相应的数据集上训练到收敛，而Dropout上大部分模型都没有显式的被训练，因为通常神经网络会很大，在单个步骤里训练一小部分子网络，参数共享使得剩余的子网络也能有很好的参数设定。 在Dropout中，通过掩码\\(\\mu\\)定义每个子模型的概率分布\\(p(y|x,\\mu)\\)，所有掩码的算术平均值为： \\[ \\sum_\\mu p(\\mu)p(y|x,\\mu) \\] 其中\\(p(\\mu)\\)是训练时采样\\(\\mu\\)的概率分布。这个求和包含多达指数级的项，除非模型能够被简化否则不可能计算出来，通常通过采样近似推断。但是更好的方法是使用集成成员的几何平均而不是算术平均，这个近似整个集成。 多个概率分布的几何平均不能保证是一个概率分布，为了保证结果是一个概率分布，要求没有子模型给某一事件分布的概率为0，并重新标准化所得分布，通过几何平均直接定义的非标准化概率分布如下： \\[ \\tilde{p}_{ensenmble}(y|x)=\\sqrt[2^d]{\\prod_\\mu p(y|x,\\mu)} \\] 其中d是可被丢弃的单元数，为了进行预测，需重新进行标准化： \\[ p_{ensemble}(y|x)=\\frac{\\tilde{p}(y|x)}{\\sum\\nolimits_{y&#39;}\\tilde{p}_{ensemble}(y&#39;)x} \\] 我们可以通过评估模型中的\\(p(y|x)\\)来近似\\(p_{ensemble}\\)，将单元\\(i\\)的输出权重乘以单元\\(i\\)的被包含概率，目的是得到该单元输出的正确期望，这种方法被称为权重比例推断。(详见Deep Learning书中P163) Dropout比其他标准的计算开销小的正则化更有效，Dropout也可以与其他正则化一起使用，其优点是： 1）计算方便，训练过程中参数n个随机二进制数与状态相乘，每个样本每次更新只需\\(O(n)\\)的计算复杂度。 2）不限制适用的模型或训练过程，几乎在所有分布式表示且可以用随机梯度下降训练的模型上都表现良好。 Dropout是一个正则化技术，它减小了模型的有效容量，为了抵消这个影响必须增大模型规模，使用Dropout时最佳验证集的误差会低很多，但这是以更大的模型和更多的训练算法迭代次数换来的，对于非常大的数据集，正则化带来的泛化误差减小得很小，在这些情况下使用Dropout和更大的模型的计算代价可能超过正则化带来的好处。 此外Dropout不仅仅是训练一个Bagging的集成模型，而且是共享隐藏单元的集成模型，这意味着每个隐藏单元不管是否在模型中，都必须能表现良好，因此每个隐藏单元不仅是一个很好的特征，更要在许多情况下表现良好。 Dropout强大的大部分原因是其施加了在隐藏层的掩码噪声，可以看作对输入内容的信息高度智能化、自适应破坏的一种形式，破坏提取的特征而不是破坏原始值，让破坏过程充分利用该模型获得的关于输入分布的所有知识。","categories":[{"name":"Deep Learning Book","slug":"Deep-Learning-Book","permalink":"http://blog.tifosi-m.com/categories/Deep-Learning-Book/"}],"tags":[{"name":"Deep Learning","slug":"Deep-Learning","permalink":"http://blog.tifosi-m.com/tags/Deep-Learning/"}],"keywords":[{"name":"Deep Learning Book","slug":"Deep-Learning-Book","permalink":"http://blog.tifosi-m.com/categories/Deep-Learning-Book/"}]},{"title":"《机器学习》笔记 —— 支持向量机","slug":"ML_SVM","date":"2017-08-25T08:00:00.000Z","updated":"2017-09-29T04:58:45.355Z","comments":true,"path":"2017/08/25/ML_SVM/","link":"","permalink":"http://blog.tifosi-m.com/2017/08/25/ML_SVM/","excerpt":"支持向量机基本形式 分类学习的思想基本思想是基于训练集D在样本空间找到一个划分超平面，将不同的样本分开，而SVM试图找到一个最大间隔的划分超平面。 划分超平面可通过如下线性方程来描述： \\[ w^Tx+b=0 \\] 其中，\\(w\\)为法向量，决定超平面方向，\\(b\\)为位移量决定超平面与原点的距离。 样本空间任意点\\(x\\)到超平面的距离： \\[r=\\frac{w^Tx+b}{||w||}\\] 假设超平面\\((w,b)\\)能将训练样本正确分类，即对于\\((x_i,y_i)\\in D)\\)，若\\(y_i=+1\\)，则\\(w^T_i+b&gt;0\\)，若\\(y_i=-1\\)则有\\(w^Tx_i+b&lt;0\\)，令 \\[ \\left \\{ \\begin{array}{rl} w^Tx_i+b\\geq +1 \\text{, }y_i=+1 \\\\ \\\\ w^Tx_i+b\\leq -1 \\text{, }y_i=-1 \\end{array} \\right . \\] 距离超平面最近的几个训练样本使上式等号成立，它们被称为“支持向量”，两个异类支持向量到超平面的距离之和： \\[ \\gamma=\\frac{2}{||w||} \\] 最大间隔，即找到\\(w,b\\)，使得\\(\\gamma\\)最大： \\[ \\max_{w,b}\\frac{2}{||w||} \\\\ s.t.\\quad \\, y_i(w^Tx_i+b)\\geq 1 \\text{, }i=1,2,\\dots,m \\] 即， \\[ \\min_{w,b}\\frac{1}{2}||w||^2 \\\\ s.t.\\quad y_i(w^Tx_i+b)\\geq 1 \\text{, }i=1,2,\\dots,m \\] 此为支持向量机(SVM)的基本型。","text":"支持向量机基本形式 分类学习的思想基本思想是基于训练集D在样本空间找到一个划分超平面，将不同的样本分开，而SVM试图找到一个最大间隔的划分超平面。 划分超平面可通过如下线性方程来描述： \\[ w^Tx+b=0 \\] 其中，\\(w\\)为法向量，决定超平面方向，\\(b\\)为位移量决定超平面与原点的距离。 样本空间任意点\\(x\\)到超平面的距离： \\[r=\\frac{w^Tx+b}{||w||}\\] 假设超平面\\((w,b)\\)能将训练样本正确分类，即对于\\((x_i,y_i)\\in D)\\)，若\\(y_i=+1\\)，则\\(w^T_i+b&gt;0\\)，若\\(y_i=-1\\)则有\\(w^Tx_i+b&lt;0\\)，令 \\[ \\left \\{ \\begin{array}{rl} w^Tx_i+b\\geq +1 \\text{, }y_i=+1 \\\\ \\\\ w^Tx_i+b\\leq -1 \\text{, }y_i=-1 \\end{array} \\right . \\] 距离超平面最近的几个训练样本使上式等号成立，它们被称为“支持向量”，两个异类支持向量到超平面的距离之和： \\[ \\gamma=\\frac{2}{||w||} \\] 最大间隔，即找到\\(w,b\\)，使得\\(\\gamma\\)最大： \\[ \\max_{w,b}\\frac{2}{||w||} \\\\ s.t.\\quad \\, y_i(w^Tx_i+b)\\geq 1 \\text{, }i=1,2,\\dots,m \\] 即， \\[ \\min_{w,b}\\frac{1}{2}||w||^2 \\\\ s.t.\\quad y_i(w^Tx_i+b)\\geq 1 \\text{, }i=1,2,\\dots,m \\] 此为支持向量机(SVM)的基本型。 对偶问题 求解SVM得到大间隔划分超平面所对应的模型： \\[f(x)=w^Tx+b\\] 对SVM基本型使用拉格朗日乘子法，则该问题的拉格朗日函数可写为： \\[ L(w,b,\\alpha)=\\frac{1}{2}||w||^2+\\sum_{i=1}^m\\alpha_i(1-y_i(w^Tx_i+b)) \\qquad (1) \\] 其中\\(\\alpha_i \\geq 0\\) 因为： \\[ \\max_{\\alpha_i} L(w,b,\\alpha)=\\frac{1}{2}||w||^2 \\] 所以SVM原问题可以表示为： \\[ \\min_{w,b}\\max_{\\alpha_i} L(w,b,\\alpha) \\] 令\\(L(w,b,\\alpha)\\)对\\(w\\text{和}b\\)的偏导为零： \\[ \\begin{array}{} w =\\sum_{i=1}^m\\alpha_iy_ix_i \\qquad &amp;(2) \\\\ 0 =\\sum_{i=1}^m\\alpha_iy_i \\qquad &amp;(3) \\end{array} \\] 将(2)式代入(1)式，再考虑(3)式的约束，则得SVM得对偶问题： \\[ \\max_\\alpha \\sum_{i=1}^m\\alpha_i-\\frac{1}{2}\\sum_{i=1}^m\\sum_{j=1}^m\\alpha_i\\alpha_jy_iy_jx_i^Tx_j \\qquad (4)\\\\ s.t.\\quad \\sum_{i=1}^m\\alpha_iy_i=0 \\qquad \\alpha_i\\geq 0 \\text{, }i=1,2,\\dots,m \\] 解得\\(\\alpha\\)后得： \\[ f(x)=w^Tx+b=\\sum_{i=1}^m\\alpha_iy_ix_i^Tx+b \\qquad (5) \\] 因SVM存在不等式约束，因此上述过程满足KKT条件，即 \\[ \\left \\{ \\begin{array}{} \\alpha_i \\geq 0 \\\\ \\\\ y_if(x_i)-1 \\geq 0 \\\\ \\\\ \\alpha_i(y_if(x_i)-1)=0 \\end{array} \\right. \\] 对任意\\((x_i,y_i)\\),总有\\(\\alpha_i=0\\)或\\(y_if(x_i)=1\\)，若\\(\\alpha_i=0\\)，则该样本不会在(5)式中出现，也不会影响\\(f(x)\\)，若\\(\\alpha_i &gt;0\\)，则\\(y_if(x_i)=1\\)，则该样本位于最大间隔边界上，是一个支持向量。（训练完成后，大部分训练样本都不需要保留。） SVM为什么使用对偶问题求解？ 1）原问题求解在样本空间维度很大时求解困难。 2）在解决非线性问题时，需要通过核函数映射到高纬空间，这使得求解更加困难，而使用对偶问题后，复杂度只与样本数量有关。 SMO算法 SMO算法的基本思想是先固定\\(\\alpha_i\\)之外的所有参数，然后求\\(\\alpha_i\\)上的极值，由于存在约束\\(\\sum_{i=1}^m\\alpha_iy_i=0\\)，则固定\\(\\alpha_i\\)之外的其他变量，\\(\\alpha_i\\)可由其他变量导出，于是SMO每次选择两个变量\\(\\alpha_i\\text{和}\\alpha_j\\)，并固定其他参数，SMO不断迭代如下两个步骤直至收敛： 1）选取一对需要更新的变量\\(\\alpha_i\\text{和}\\alpha_j\\) 固定\\(\\alpha_i\\text{和}\\alpha_j\\)以外的参数，求解(5)式获得更新后的\\(\\alpha_i\\text{和}\\alpha_j\\) 只要选取的\\(\\alpha_i\\text{和}\\alpha_j\\)中有一个不满足KTT条件，目标函数就会在迭代后减小，KTT条件违背的程度越大，则变量更新后可能导致的目标函数值减幅越大。SMO采取启发式选取：使选取的两个变量所对应的样本间隔最大。 SMO优化两个参数的过程非常高效，因为仅考虑\\(\\alpha_i\\text{和}\\alpha_j\\)的情况是，上述(4)式中的约束条件可改写成： \\[ \\alpha_iy_i+\\alpha_jy_i=c \\text{, } \\qquad \\alpha_i \\geq 0,\\alpha_j \\geq 0 \\] 其中： \\[ c = -\\sum_{k\\neq i,j}\\alpha_ky_k \\] 则根据\\(\\alpha_iy_i+\\alpha_jy_i=c\\)可消去\\(\\alpha_j\\)，得到关于\\(\\alpha_i\\)的单变量二次规划问题，此问题拥有闭式解，可以高效更新\\(\\alpha_i\\text{和}\\alpha_j\\) 又对任意一个支持向量\\((x_s,y_s)\\)，根据KTT条件可知\\(y_sf(x_s)=1\\)，所以： \\[ y_s(\\sum_{i\\in S}\\alpha_iy_ix_i^Tx_s+b)=1 \\] 其中\\(S\\)为所有支持向量的下标集合，通常根据所有支持向量求解的平均值解得b的值。 核函数 原始样本空间内也许不存在一个能正确划分两类样本的超平面，对于这样的问题，可将样本从原始空间映射到一个更高维的特征空间，如果原始空间是有限维，那一定存在一个高维特征空间使样本可分。 令\\(\\phi(x)\\)表示将\\(x\\)映射后的特征向量，则划分超平面对应的模型为： \\[ f(x)=w^T\\phi(x)+b \\] 则： \\[ \\min_{a,b}\\frac{1}{2}||w||^2 \\\\ s.t. \\quad y_i(w^T\\phi(x_i))+b \\geq 1 \\text{, }i=1,2,\\dots,m \\] 其对偶问题： \\[ \\min_\\alpha \\sum_{i=1}^m \\alpha_i-\\frac{1}{2}\\sum_{i=1}^m\\sum_{j=1}^m \\alpha_i\\alpha_jy_iy_j\\phi(x_i)^T\\phi(x_j) \\qquad (6) \\\\ s.t. \\quad \\sum_{i=1}^m \\alpha_iy_i=0 \\text{, } \\quad \\alpha_i \\geq 0 \\text{, }i=1,2,\\dots,m \\] 由于特征空间维数可能很高，直接计算\\(\\phi(x_i)^T\\phi(x_j)\\)通常很困难，为了避开这个障碍，则可令： \\[ \\kappa(x_i,x_j)=\\langle\\phi(x_i),\\phi(x_j)\\rangle=\\phi(x_i)^T\\phi(x_j) \\] 即\\(x_i\\)与\\(x_j\\)在特征空间的内积等于它们在原始空间通过\\(\\kappa(\\cdot,\\cdot)\\)计算的结果。 因此(6)式可改为： \\[ \\max_\\alpha \\sum{i=1}^m\\alpha_i-\\frac{1}{2}\\sum_{i=1}^m\\sum_{j=1}^m\\alpha_i\\alpha_jy_iy_j\\kappa(x_i,x_j) \\\\ s.t. \\quad \\sum_{i=1}^m\\alpha_iy_i=0 \\text{, } \\quad \\alpha_i \\geq 0 \\text{, }i=1,2,\\dots,m \\] 求解得： \\[ \\begin{array}{} f(x) &amp; =w^T\\phi(x)+b \\\\ &amp; = \\sum_{i=1}^m\\alpha_iy_i\\phi(x_i)^T\\phi(x)+b \\\\ &amp; = \\sum_{i=1}^m\\alpha_iy_i\\kappa(x_i,x_j)+b \\end{array} \\qquad (7) \\] \\(\\kappa(\\cdot,\\dot)\\)称为核函数，式(7)又称“支持向量展式” 常用核函数： | 核函数 | 形式 | 说明 | | :——–: |:—–: | :—-: | | 线性核 | \\(\\kappa(x_i,x_j)=x_i^Tx^j\\) | | | 多项式核 | \\(\\kappa(x_i,x_j)=(x_i^Tx_j)^d\\) | \\(d \\geq 1\\)为多项式的次数 | | 高斯核 | \\(\\kappa(x_i,x_j)=exp(-\\frac{||x_i-x_j||^2}{2\\sigma^2})\\) | \\(\\sigma &gt; 0\\)为高斯核带宽 | |拉布拉斯核 | \\(\\kappa(x_i,x_j)=exp(-\\frac{||x_i-x_j||}{\\sigma^2})\\) |\\(\\sigma &gt; 0\\)| |sigmoid核 | \\(\\kappa(x_i,x_j)=tanh(\\beta x_i^Tx_j + \\theta)\\) | \\(\\beta&gt;0,\\theta&lt;0\\)| 此外还可进行函数组合： \\[ \\gamma_1\\kappa_1+\\gamma_2\\kappa_2 \\text{、} \\kappa_1 \\otimes \\kappa_2(x,z)=\\kappa_1(x_1,z)\\kappa_2(x_2,z) \\text{、} \\kappa(x,z)=g(x)\\kappa_1(x,z)g(z) \\] 软间隔与正则化 软间隔允许支持向量机在一些样本出错，即软间隔允许某些样本不满足约束：\\(y_i(w^Tx_i+b) \\geq 1\\) 优化目标： \\[ \\min_{a,b}\\frac{1}{2}||w||^2+C\\sum_{i=1}^m\\ell_{0/1}(y_i(w^Tx_i+b)-1) \\] 其中C&gt;0，\\(\\ell_{0/1}\\)是0/1损失函数： \\[ \\ell_{0/1}(z)= \\left \\{ \\begin{array}{} 1 \\text{, if }z &lt; 0 \\\\ \\\\ 0 \\text{, otherwise} \\end{array} \\right . \\] 替代损失： hige损失：\\(\\ell_{hinge}(z)=\\max(0,1-z)\\) 指数损失：\\(\\ell_{exp}(z)=exp(-z)\\) 对率损失：\\(\\ell_{\\log}(z)=\\log(1+exp(-z))\\) 引入“松弛变量”\\(\\xi \\geq 0\\)，则 \\[ \\min_{w,b,\\xi_i} \\frac{1}{2}||w||^2+C\\sum_{i=1}^m \\xi_i \\\\ s.t. \\quad y_i(w^Tx_i+b) \\geq 1-\\xi_i \\text{, } \\quad \\xi \\geq 0 \\text{, }i=1,2,\\dots,m \\] 亦称“软间隔支持向量机” 如果使用对率损失函数\\(\\ell_{\\log}\\)替换0/1损失函数，则几乎得到逻辑回归模型，实际上SVM与逻辑回归的优化目标相近，性能通常也相当，但逻辑回归能直接用于多分类任务。 由于hinge损失有一块“平坦”的零区域，使得SVM的解具有稀疏性，而对率损失是光滑的单调递减函数，不能导出支持向量的概念，故逻辑回归的解依赖更多训练样本。 SVM更一般的形式： \\[ \\min_f\\Omega(f)+C\\sum_{i=1}^m\\ell(f(x_i),y_i) \\] \\(\\Omega\\)为结构风险(描述\\(f\\)的性质)，\\(\\ell\\)为经验风险(描述模型与训练数据契合程度)，C用来进行折中。 从经验风险最小化的角度，\\(\\Omega(f)\\)表示获得具有何种性质的模型，另一方面，改信息有助于消减假设空间，降低过拟合风险，上式称为正则化问题，\\(\\Omega\\)称为正则项，C称为正则化系，\\(L_p\\)范数为常用正则化项。 \\(L_2\\)范数：w的分量取尽量均衡，即非零向量尽量稠密。 \\(L_1\\)范数：w的分量尽量稀疏，即非零分量个数尽量少。 支持向量回归(SVR) 假设能容忍\\(f(x)\\)与\\(y\\)之间最多有\\(\\epsilon\\)的偏差，即仅当\\(|f(x)-y|&gt;\\epsilon\\)时才计算损失。 SVR问题形式为： \\[ \\min_{w,b}\\frac{1}{2}||w||^2+C\\sum_{i=1}^m\\ell_\\epsilon(f(x_i)-y_i) \\] 其中C为正则化常数，\\(\\ell_\\epsilon\\)为\\(\\epsilon\\)不敏感损失函数。 \\[ \\ell_\\epsilon(z)= \\left \\{ \\begin{array}{} 0 \\qquad &amp; \\text{if } |z| \\leq \\epsilon \\\\ \\\\ |z|-\\epsilon &amp; \\text{otherwise} \\end{array} \\right . \\] 引入松弛变量\\(\\xi_i,\\hat \\xi_i\\): \\[ \\min_{w,b,\\xi_i,\\hat \\xi_i}\\frac{1}{2}||w||^2+C\\sum_{x=1}^m(\\xi_i+\\hat \\xi_i) \\\\ \\begin{array}{} s.t. \\quad &amp; f(x_i)-y_i \\leq \\epsilon + \\xi_i \\\\ &amp; y_i-f(x_i) \\leq \\epsilon + \\hat \\xi_i \\\\ &amp; \\xi_i \\geq 0,\\quad \\hat \\xi_i \\geq 0 , \\quad i=1,2,\\dots,m \\end{array} \\] 其解法与SVM类似。","categories":[{"name":"Meachine Learning","slug":"Meachine-Learning","permalink":"http://blog.tifosi-m.com/categories/Meachine-Learning/"}],"tags":[{"name":"Meachine Learning","slug":"Meachine-Learning","permalink":"http://blog.tifosi-m.com/tags/Meachine-Learning/"}],"keywords":[{"name":"Meachine Learning","slug":"Meachine-Learning","permalink":"http://blog.tifosi-m.com/categories/Meachine-Learning/"}]},{"title":"《机器学习》笔记 —— 线性模型","slug":"ML_linear model","date":"2017-08-22T10:00:00.000Z","updated":"2017-09-29T04:58:45.355Z","comments":true,"path":"2017/08/22/ML_linear model/","link":"","permalink":"http://blog.tifosi-m.com/2017/08/22/ML_linear model/","excerpt":"基本形式： 给定d个属性的示例\\(x=(x_1,x_2,\\dots ,x_d)\\)，线性模型试图学得一个通过属性的线性组合来进行预测的函数： \\[f(x)=w_1x_1+w_2x_2+,\\dots ,w_dx_d+b\\] 一般向量形式： \\[f(x)=W^Tx+b\\] 线性回归 给定数据集\\(D=\\{(x_1,y_1),(x_2,y_2),\\dots ,(x_m,y_m)\\}\\)，其中\\(x_i=(x_{i1},\\dots ,x_{id})\\)，\\(y_i\\in \\mathbb{R}\\)，线性回归试图学得一个线性模型以尽可能准确预测实值输出标记。 线性回归试图学得\\(f(x)=w^Tx+b\\approx y_i\\)，以均方误差作为性能度量，目标是使均方误差最小，即： \\[(w^*,b^*)=\\arg\\min_{w,b}\\sum_{i=1}^m(f(x_i)-y_i)^2\\] 基于均方误差最小化来进行建模，求解的方法为“最小二乘法”，其试图找到一条直线，使所有样本到直线的距离最小。","text":"基本形式： 给定d个属性的示例\\(x=(x_1,x_2,\\dots ,x_d)\\)，线性模型试图学得一个通过属性的线性组合来进行预测的函数： \\[f(x)=w_1x_1+w_2x_2+,\\dots ,w_dx_d+b\\] 一般向量形式： \\[f(x)=W^Tx+b\\] 线性回归 给定数据集\\(D=\\{(x_1,y_1),(x_2,y_2),\\dots ,(x_m,y_m)\\}\\)，其中\\(x_i=(x_{i1},\\dots ,x_{id})\\)，\\(y_i\\in \\mathbb{R}\\)，线性回归试图学得一个线性模型以尽可能准确预测实值输出标记。 线性回归试图学得\\(f(x)=w^Tx+b\\approx y_i\\)，以均方误差作为性能度量，目标是使均方误差最小，即： \\[(w^*,b^*)=\\arg\\min_{w,b}\\sum_{i=1}^m(f(x_i)-y_i)^2\\] 基于均方误差最小化来进行建模，求解的方法为“最小二乘法”，其试图找到一条直线，使所有样本到直线的距离最小。 以\\(\\hat w=(w;b)\\)，相应的数据集D表示为\\(m\\text{x}(d+1)\\)的矩阵\\(X\\)，则 \\[\\hat w=\\arg\\min_{\\hat w}(y-X\\hat w)^T(y-X\\hat w)\\] 令\\(E_{\\hat w}=(y-X\\hat w)^T(y-X\\hat w)\\)，又\\(\\hat w\\)求导得： \\[\\frac{\\partial E_\\hat w}{\\partial \\hat w}=2X^T(X\\hat w-y)\\] 当\\(X^TX\\)为满秩矩阵或正定矩阵，令上式为0可得： \\[\\hat w^*=(X^TX)^{-1}X^Ty\\] 则令\\(\\hat x_i=(x_i,1)\\) \\(f(\\hat x_i)=\\hat x_i^T(X^TX)^{-1}X^Ty\\) 若\\(X^TX\\)不为满秩，则\\(\\hat w^*\\)将有多组解，通常由学习算法的归纳偏好决定最终解，常见作法是引入正则化。 假设我们认为示例所对应的输出是在指数尺度上变化，则衍生出对数线性回归。 对数线性回归：\\(\\ln y=w^Tx+b\\) 实际上试图让\\(e^{w^Tx+b}\\)逼近y。 更一般的，单调可微函数\\(g(\\cdot)\\)，令\\(y=g^{-1}(w^Tx+b)\\)，称为广义线性模型，\\(g(\\cdot)\\)称为“联系函数”。 逻辑回归 针对分类任务，只需找一个单调可微函数将分类任务的真实标记与线性回归的预测值联系起来。 单位阶跃函数： \\[y = \\left\\{ \\begin{array}{rl} 0 &amp; \\text{if } z &lt; 0 \\\\ 0.5 &amp;\\text{if } z = 0 \\qquad\\text{(Heaviside函数)}\\\\ 1 &amp; \\text{if } z &gt; 0 \\end{array} \\right. \\] 由于单位阶跃函数不连续，不能直接作为\\(g^{-1}(\\cdot)\\) logistic函数： \\[y=\\frac{1}{1+e^{-z}} \\qquad\\text{(sigmoid)}\\] 即： \\[y=\\frac{1}{1+e^{-(w^Tx+b)}}\\] 两边取对数，得到对数几率： \\[\\ln\\frac{y}{1-y}=w^Tx+b\\] 那么如何确定\\(w\\text{和}b\\)呢？将\\(y\\)视为类后验概率\\(P(y=1|x)\\)则， \\[ \\ln\\frac{p(y=1|x)}{p(y=0)|x}=w^T+b \\\\ \\Rightarrow \\qquad p(y=1|x)=\\frac{e^{w^Tx+b}}{1+e^{w^Tx+b}} \\quad p(y=0|x)=\\frac{1}{e^{w^Tx+b}} \\] 通过最大似然估计\\(w\\text{和}b\\)给定数据集\\(\\{(x_i,y_i)\\}_{i=1}^m\\)，最大化对数似然： \\[\\ell(w,b)=\\sum_{i=1}^m\\ln p(y_i|x_i;w,b)\\] 令\\(\\beta=(w,b),\\hat x=(x,1)\\)则\\(w^T+b\\)为\\(\\beta^T\\hat x\\)，再令\\(p_1(\\hat x;\\beta)=p(y=1|\\hat x;\\beta)\\),\\(p_0(\\hat x;\\beta)=p(y=0|\\hat x;\\beta)\\)则： \\[p(y_i|x_i;w,b)=y_ip_i(\\hat x;\\beta)+(1-y_i)p_0(\\hat x_i;\\beta)\\] 最大化转化为最小化： \\[\\ell(\\beta)=\\sum_{i=1}^m(-y_i\\beta^T\\hat x_i)+\\ln(1+e^{\\beta^T\\hat x_i})\\] 由经典数值优化算法，梯度下降、牛顿法等可解其最优解。 线性判别分析(LDA) LDA思想：设法将样例投影到一条直线上，使同类样例的投影点尽可能的近，异类样例的投影点尽可能远。 给定数据集\\(D=\\{(x_i,y_i)\\}_{i=1}^m,y_i\\in\\{0,1\\}\\),令\\(X_i,\\mu_i,\\Sigma_i\\)，分别表示第\\(i\\in\\{0,1\\}\\)类示例集合、均值向量、协方差矩阵。 若将数据投影在直线\\(w\\)上，则两类样本的中心在直线上的投影分别为\\(w^T\\mu_0\\text{和}w^T\\mu_1\\)，两类样本的协方差为\\(w^T\\Sigma_0w\\text{和}w^T\\Sigma_1w\\)，直线是一维空间，故\\(w^T\\mu_0、w^T\\mu_1、w^T\\Sigma_0w\\text{和}w^T\\Sigma_1w\\)均为实数。 同类样例投影点近，即同类样例投影点的协方差尽可能小，异类样例投影点尽可能远即类中心距离尽可能大，综合考虑最大化： \\[ \\begin{array}{rl} J &amp; = \\frac{||w^T\\mu_0-w^T\\mu_1||^2}{w^T\\Sigma_0w+w^T\\Sigma_1w} \\\\ &amp; = \\frac{w^T(\\mu_0-\\mu_1)(\\mu_0-\\mu_1)^Tw}{w^T(\\Sigma_0+\\Sigma_1)w} \\end{array} \\] 定义类内散度矩阵： \\[ \\begin{array}{rl} S_w &amp; = \\Sigma_0+\\Sigma_1 \\\\ &amp; = \\sum_{x\\in X_0}(x-\\mu_0)(x-\\mu_0)^T+\\sum_{x\\in X_1}(x-\\mu_1)(x-\\mu_1)^T \\end{array} \\] 类间散度矩阵 \\[ S_b = (\\mu_0-\\mu_1)(\\mu_0-\\mu_1)^T \\] 则， \\[ J = \\frac{w^TS_bw}{w^TS_ww} \\qquad \\text{(广义瑞利商)} \\] 令\\(w^TS_ww=1\\)，则得： \\[ \\min_w-w^TS_bw \\quad \\text{s.t.}w^TS_ww=1 \\] 由拉格朗日乘子法，上式等价于 \\[S_bw=\\lambda S_ww\\] 又因\\(S_bw\\)的方向恒为\\(\\mu_0-\\mu_1\\)，不妨令\\(S_bw=\\lambda(\\mu_0-\\mu_1)\\)，则代入上式得 \\[ w=S_w^{-1}(\\mu_0-\\mu_1) \\] 因数值解的稳定性，对\\(S_w\\)进行奇异值分解，即\\(S_w=V\\Sigma^{-1}U^T\\)，得到\\(S_w^{-1}=V\\Sigma^{-1}U^T\\) 多类LDA 假定存在N个类，且第i类示例为\\(m_i\\)，定义“全局散度矩阵”： \\[ \\begin{array}{rl} S_t &amp; = S_b+S_w \\\\ &amp; = \\sum_{i=1}^m(x_i-\\mu)(x_i-\\mu)^T \\end{array} \\] 类内散度矩阵\\(S_w\\)重定义为每个类别的散度矩阵之和： \\[ S_w=\\sum_{i=1}^NS_{wi} \\] \\[ \\begin{array} \\text{其中} \\qquad \\qquad S_{wi} &amp; =\\sum_{x\\in X_i}(x-\\mu_i)(x-\\mu_i)^T \\\\ \\text{所以} \\qquad \\qquad S_b &amp; = S_t-S_b \\\\ &amp; = \\sum_{i=1}^Nm_i(\\mu_i-\\mu)(\\mu_i-\\mu)^T \\end{array} \\] 常见优化目标： \\[ \\max_W\\frac{tr(W^TS_bW)}{W^TS_wW} \\] 其中\\(W\\in \\mathbb{R}^{dx(N-1)}\\)，\\(tr(\\cdot)\\)为矩阵的迹。通过广义特征值问题求解： \\[ S_bW = \\lambda S_wW \\] \\(W\\)的闭式解则是\\(S_w^{-1}S_b\\)的最大广义特征值所对应的特征向量组成的矩阵。 若将\\(W\\)视为投影矩阵，则多类LDA将投影到\\(N-1\\)维空间，\\(N-1\\)通常远小于原有属性数，且投影过程使用了类别信息，所以LDA也常被视为经典的监督降维技术。 多分类学习 利用二分类来解决多分类问题 经典拆分策略： 1）一对一：将N个类别两两配对，从而产生N(N-1)/2个二分类任务，测试阶段将新样本交给所有分类器，最终结果通过投票产生。 2）一对其余：训练N个分类器，在测试阶段若仅有一个分类器预测为正类，则对应类别标记为最终分类结果，否则要考虑各分类器的预测置信度。 3）多对多：每次讲若干个类作为正类，若干个类作为反类，正类、反类构造需特别设计，常用的技术：“纠错输出码”(ECOC)。","categories":[{"name":"Meachine Learning","slug":"Meachine-Learning","permalink":"http://blog.tifosi-m.com/categories/Meachine-Learning/"}],"tags":[{"name":"Meachine Learning","slug":"Meachine-Learning","permalink":"http://blog.tifosi-m.com/tags/Meachine-Learning/"}],"keywords":[{"name":"Meachine Learning","slug":"Meachine-Learning","permalink":"http://blog.tifosi-m.com/categories/Meachine-Learning/"}]},{"title":"《Deep Learning》笔记 —— 深度前馈网络","slug":"Deep_Feedforward_Networks","date":"2017-08-20T13:00:00.000Z","updated":"2017-09-29T04:58:45.351Z","comments":true,"path":"2017/08/20/Deep_Feedforward_Networks/","link":"","permalink":"http://blog.tifosi-m.com/2017/08/20/Deep_Feedforward_Networks/","excerpt":"深度前馈网络也叫作前馈神经网络或者多层感知机，一种理解前馈网络的方式是从线性模型开始的，并考虑克服其局限性。线性模型，如逻辑回归和线性回归，无论是通过闭解形式还是使用凸优化，它们都能高效且可靠的拟合。但是也存在明显缺陷，线性模型被局限在线性函数里，所以无法理解两个输入变量的相互作用。 为了扩展线性模型来表示\\(x\\)的非线性函数，引入非线性变换\\(\\phi(x)\\)，关于如何选择\\(\\phi\\)，有如下方法： 1）使用一个通用\\(\\phi\\)，例如无限维的\\(\\phi\\)，它隐含地用在基于RBF核的核机器上。非常通用的特征映射通常只基于局部光滑原则，并且没有足够先验信息解决高级问题。 2）手动设计\\(\\phi\\)，缺点是不同的领域难以迁移。 3）深度学习的策略是自主学习\\(\\phi\\)，这种方法的优点是我们只需要寻找正确的函数族就可以了，不需要去寻找精确函数。","text":"深度前馈网络也叫作前馈神经网络或者多层感知机，一种理解前馈网络的方式是从线性模型开始的，并考虑克服其局限性。线性模型，如逻辑回归和线性回归，无论是通过闭解形式还是使用凸优化，它们都能高效且可靠的拟合。但是也存在明显缺陷，线性模型被局限在线性函数里，所以无法理解两个输入变量的相互作用。 为了扩展线性模型来表示\\(x\\)的非线性函数，引入非线性变换\\(\\phi(x)\\)，关于如何选择\\(\\phi\\)，有如下方法： 1）使用一个通用\\(\\phi\\)，例如无限维的\\(\\phi\\)，它隐含地用在基于RBF核的核机器上。非常通用的特征映射通常只基于局部光滑原则，并且没有足够先验信息解决高级问题。 2）手动设计\\(\\phi\\)，缺点是不同的领域难以迁移。 3）深度学习的策略是自主学习\\(\\phi\\)，这种方法的优点是我们只需要寻找正确的函数族就可以了，不需要去寻找精确函数。 1. 基于梯度的学习 线性模型和神经网络最大的区别在于神经网络的非线性导致大多数我们感兴趣的代价函数都变得非凸。这意味着神经网络的训练通常使用迭代的、基于梯度的优化，仅仅使得代价函数达到一个非常小的值，而不是想训练线性回归模型、SVM的凸优化那样保证全局收敛。 1.1 代价函数 大多数现代神经网络使用最大似然来训练，这意味着代价函数是负对数似然，它与训练数据和模型分布间的交叉熵等价，这个代价函数表示为： \\[J(\\theta)=-\\mathbb{E}_{x,y \\sim \\hat{p}_{data}}log\\,p_{model}(y|x)\\] 使用最大似然来导出代价函数的方法的一个优势是，它减轻了为每个模型设计代价函数的负担。明确一个模型的\\(p(y|x)\\)则自动地确定了一个代价函数\\(log\\,p(y|x)\\)。 特别的，代价函数的梯度必须足够的大和具有足够的预测性，而饱和的函数把梯度变得非常小破坏了这个目标。值得注意的是这种情况在神经网络中经常发生，因为隐藏层或输出层的激活函数会饱和。负对数似然能够避免这个问题，因为负对数代价函数中的对数函数消除了某些输出单元的指数效果。 1.2 输出单元 高斯输出分布的线性单元 给定特征\\(h\\)，线性输出单元产生一个向量\\(\\hat{y}=W^Th+b\\)。线性输出层经常用来产生条件高斯分布的均值： \\[p(y|x)=\\mathcal{N}(y;\\hat{y},I)\\] 最大化其对数似然等价于最小化均方误差。 因为线性单元不会饱和，所以易于采用基于梯度的优化算法。 伯努利输出分布的sigmoid单元 sigmoid输出单元定义为\\(\\hat{y}=\\sigma(w^Th+b)\\)，可将其分为两个部分，使用线性层来计算\\(z=w^Th+b\\)，接着使用sigmoid激活函数将\\(z\\)转化为概率。 当我们使用其他损失函数，损失函数将在\\(\\sigma(z)\\)饱和时饱和，sigmoid在\\(z\\)取非常小的负值时会饱和到0，取非常大的正值时会饱和到1，这种情况一旦发生，梯度会变得非常小以至于不能用来学习，因此最大似然几乎总是训练sigmoid输出单元的优选方案。 多项式输出分布的softmax单元 任何时候，当需要表示一个具有n个可能取值的离散随机变量的分布时，都可以使用softmax函数，softmax函数最常用作分类器的输出，来表示n个不同类上的概率分布。用于伯努利分布的方法可以推广到多项式分布。首先线性层预测未归一化的对数概率： \\[z=W^Th+b\\] 其中\\(z_i=log\\,\\hat{P}(y=i|x)\\)，softmax函数对z指数化和归一化来获得需要的\\(\\hat{y}\\)，其函数形式为： \\[softmax(z)_i=\\frac{e^{z_i}}{\\sum_je^{z_j}}\\] 使用最大似然训练softmax输出目标y，对数似然中的log将抵消softmax中的\\(e\\)： \\[log\\,softmax(z_i)=z_i-log\\,\\sum_{j}e^{z_j}\\] 该式中的第一项\\(z_i\\)不会饱和，所以即使\\(z_i\\)对第二项贡献很小，学习依然可以进行。此外第二项\\(log\\,\\sum_je^{z_j}\\)可以近似为\\(\\max\\limits_j\\,z_j\\)，如果正确答案已经具有softmax的最大输入则\\(-z_i\\)与\\(log\\,\\sum_j \\approx \\max\\limits_j\\,z_j=z_j\\)将大致抵消。这样整个样本对整体训练代价贡献很小，所以这个代价将主要由为被正确分类的样本产生。 对数似然之外的许多目标函数对softmax函数不起作用，因为指数函数的变量取非常小的负值时将造成梯度消失，特别是平方误差，对softmax单元来说它是很差的损失函数。 其他的输出类型 最大似然原则给如何为任何种类的输出层设计一个好的代价函数提供了指导。 2. 隐藏单元 ReLU单元是隐藏单元极好的默认选择，一些隐藏层单元可能并不是在所有的输入点上都是可微的，例如ReLU单元\\(g(z)=max{0,z}\\)在\\(z=0\\)处不可微，在实践中，梯度下降对这些机器学习模型仍然表现良好，部分原因是因为神经网络算法通常不会达到代价函数的局部最小值，而是仅仅显著减小它的值。 2.1 ReLU单元的扩展 ReLU单元易于优化，因为其与线性单元非常类似，ReLU单元在其一半的定义域上输出为零，这使得ReLU单元只要处于激活状态，其导数都比较大和一致，而且二阶导数几乎处处为0，处于激活状态时候，它的一阶导数处处为1，所以它的梯度方向对学习来说更有用。 ReLU的一个缺陷是不能通过基于梯度的方法学习那些使他它们激活为0的样本，而ReLU的各自扩展则保证了能在各个位置都接受到梯度。 ReLU的三个扩展基于当\\(z_i&lt;0\\)时使用一个非零的斜率\\(\\alpha_i\\):\\(h_i=g(z,\\alpha)_i=max(0,z_i)+\\alpha_imin(0,z_i)\\)。 1）绝对值ReLU，固定\\(\\alpha_i=-1\\)来得到\\(g(z)=|z|\\)，它用于图像中的对象识别，在寻找输入照明极性反转下不变的特征。 2）渗漏ReLU(Leaky ReLU)，将\\(\\alpha_i\\)固定为一个类似0.01的小值。 3）参数化ReLU(PReLU)，将\\(\\alpha_i\\)作为一个可学习的参数。 maxout单元，进一步扩展了ReLU，其将\\(z\\)划分为每组具有k个值得组： \\[ g(z)_i=\\max_{j\\in\\mathbb(G^i)}z_j \\] 每个maxout单元输出每组中的最大元素。这提供了一种方法来学习对输入x空间中的多个方向的响应的分段线性函数。maxout单元可以学习多大k段的分段线性凸函数，因此可视为学习激活函数本身。使用足够大的k，maxout单元可以以任意的精确度来近似任何凸函数。(此部分详见paper) 2.2 logistic sigmoid与双曲正切函数 sigmoid单元的广泛饱和性会使基于梯度的学习变得非常困难，所以现在不鼓励使用sigmoid作为前馈网络的隐藏层单元。如果必须要使用sigmoid作为激活函数，则双曲正切函数通常要比logistic sigmoid函数表现的更好，在\\(tanh(0)=0\\)而\\(\\sigma(0)=\\frac{1}{2}\\)的意义上，tanh更像单位函数(在0附近)，所以只要网络的激活保持地很小，训练深层网络类似于训练一个线性模型。 sigmoid激活函数在除前馈神经网络以外的情景中更为常见，循环神经网络、许多概率模型以及一些自编码器有一些额外的要求使得其不能使用分段激活函数。 2.3 其他隐藏层单元 1）线性隐藏单元，即完全没有使用激活函数，线性隐藏层单元提供了一种减速网络中参数数量的有效方法。 2）softmax单元，softmax单元表示具有k个可能值得离散型变量的概率分布，所以它们可以作为一种开关。 3）RBF函数，\\(h_i = e^{-\\frac{1}{\\sigma_i^2}||W_{:,i}-x||^2}\\)因对大多数x饱和到0，所以很难优化。 4）softplus函数，\\(g(a)=\\zeta(a)=log(1+e^a)\\)，ReLU的平滑版本，通常不鼓励使用softplus，通常ReLU表现更好。 5）硬双曲正切函数，\\(g(a)=\\max(-1,min(1,a))\\)，形状与tanh类似，但相比于双曲正切，hard tanh是有界的。 3. 架构设计 架构指网络的整体结构，具有多少个单元以及这些单元如何连接。 3.1 万能近似 具有隐藏层的前馈网络提供了一种万能近似框架，具体的来说，万能定理表明，一个前馈近似神经网络如果具有线性输出层和至少一层具有任何一种“挤压”性质的激活函数的隐藏层，只要给予网络足够数量的隐藏单元，则它可以以任意精度来近似任何从一个有限维空间到另一个有限维空间的Borel可测函数（在\\(\\mathbb(R)^n\\)的有界闭集上的任意连续函数时Borel可测的）。神经网络也可以近似从任何有限维离散空间映射到另一个任意函数。 万能近似定理意味着无论试图学习什么样的函数，都能通过一个足够大MLP来表示这个函数，但是不能保证训练算法一定能够学到这个函数。存在两个原因可能导致学习失败，首先，用于训练的优化算法可能找不到用于期望函数的参数值。其次，训练算法可能由于过拟合而选择了错误的函数。 总之，具有单层前馈网络足以表示任何函数，但是网络层可能大得不可实现，并且可能无法正确地学习和泛化。在很多情况下使用更深的模型能够减少表示期望函数所需的单元数量，并且减少泛化误差。 3.2 其他架构上的考虑 许多神经网络架构被开发用于特定的任务，用于计算机视觉的卷积神经网络，用于序列处理的循环神经网络等。一般来说，层不需要连接在链中，但许多架构构建了一个主链，随后又添加了额外的架构特性，例如从层i到i+2或者更高层的跳跃连接，这些跳跃连接使得梯度更容易从输出层流向更接近输入的层。 4. 反向传播 微积分中的链式法则 设\\(x\\)是实数，\\(f\\text{和}g\\)是从实数映射到实数的函数，假设\\(y=g(x)\\)并且\\(z=f(g(x))=f(y)\\)，那么链式法则是： \\[ \\frac{dz}{dx}=\\frac{dz}{dy}\\frac{dy}{dx} \\] 扩展到向量形式，\\(x\\in \\mathbb{R}^m\\)，\\(y\\in \\mathbb{R}^n\\)，\\(g\\)是从\\(\\mathbb{R}^m\\)到\\(\\mathbb{R}^n\\)的映射，\\(f\\)是从\\(\\mathbb{R}^n\\)到\\(\\mathbb{R}\\)的映射，如果\\(y=g(x)\\)，且\\(z=f(y)\\)，则 \\[ \\frac{\\partial z}{\\partial x_i}=\\sum_j \\frac{\\partial z}{\\partial y_j}\\frac{\\partial y_j}{\\partial x_i} \\] 向量记法： \\[ \\nabla_xz = (\\frac{\\partial z}{\\partial x})^T\\nabla_yz \\] 其中\\(\\frac{\\partial y}{\\partial x}\\)是\\(g\\)的\\(n\\times m\\)Jacobian矩阵。 扩展到张量，使用\\(\\nabla_\\mathbf{X}z\\)表示值\\(z\\)关于张量\\(\\mathbf{X}\\)的梯度，使用单个变量\\(i\\)来表示张量完整的索引组，令\\(\\mathbf{Y}=g(\\mathbf{X})\\)且\\(z=f(\\mathbf{Y})\\)，则 \\[ \\nabla_\\mathbf{X}z=\\sum_j(\\nabla_\\mathbf{X}Y_j)\\frac{\\partial z}{\\partial \\mathbf{Y}_j} \\] 递归的使用链式法则实现反向传播 在迭代的计算工程中存在两种策略，存储子表达式还是重新进行计算。例如，令\\(w\\in \\mathbb{R}\\)为计算图的的输入，每一步使用相同的操作函数\\(f:\\mathbb{R} \\rightarrow \\mathbb{R}\\)，即\\(x=f(w)\\)，\\(y=f(x)\\)，\\(z=f(y)\\)，为了计算\\(\\frac{\\partial z}{\\partial w}\\)得到： \\[ \\begin{array}{} &amp; \\quad \\frac{\\partial z}{\\partial w} \\\\ &amp; = \\frac{\\partial z}{\\partial y} \\frac{\\partial y}{\\partial x} \\frac{\\partial x}{\\partial w} \\\\ &amp; = f&#39;(y)f&#39;(x)f&#39;(w) \\end{array} \\] 此实现方法，仅计算\\(f(w)\\)的值一次并将它存储在变量\\(x\\)中。另一种策略则使用\\(f&#39;(f(f(w)))f&#39;(f(w))f&#39;(w)\\)，通常在存储受限时会使用。 MLP中的的反向传播计算 前向传播： 反向传播：","categories":[{"name":"Deep Learning Book","slug":"Deep-Learning-Book","permalink":"http://blog.tifosi-m.com/categories/Deep-Learning-Book/"}],"tags":[{"name":"Deep Learning","slug":"Deep-Learning","permalink":"http://blog.tifosi-m.com/tags/Deep-Learning/"}],"keywords":[{"name":"Deep Learning Book","slug":"Deep-Learning-Book","permalink":"http://blog.tifosi-m.com/categories/Deep-Learning-Book/"}]},{"title":"《Deep Learning》笔记 —— 促使深度学习发展的原因","slug":"Challenges Motivating Deep Learning","date":"2017-08-19T19:30:00.000Z","updated":"2017-09-29T04:58:45.351Z","comments":true,"path":"2017/08/19/Challenges Motivating Deep Learning/","link":"","permalink":"http://blog.tifosi-m.com/2017/08/19/Challenges Motivating Deep Learning/","excerpt":"1. 维数灾难 一组变量不同的可能配置数量会随着变量的数目的增长而指数级增长，而这种可能的配置数量远大于训练样本的数目。许多传统机器学习算法只是简单的假设在一个新点的输出大致和最接近点的输出相同。 2. 局部不变性和平滑正则化 传统机器学习算法中存在一个使用最广泛的隐式“先验”：平滑先验或称为局部不变性先验。该先验表明学习的函数不应在小区域内发生很大的变化。 许多不同的方法显式或隐式地表示学习函数应具有平滑或局部先验，其旨在鼓励学习过程能学习出函数\\(f^*\\)，对于大多数\\(x\\)和小变动\\(\\epsilon\\)都满足条件: \\[ f^*\\approx f^*(x+\\epsilon) \\] 局部不变方法的一个极端例子是k-最近邻系列算法，当一个区域内的所有点\\(x\\)在训练集中的\\(k\\)个近邻的结果是一样的，则对这些点的预测也是一样的。 大部分核机器也是和附近训练样本相关的训练集的输出进行插值，例如一类重要的核函数是局部核(local kernel)。 决策树也具有平滑学习的局限性，因为它将输入空间分成和叶节点一样多的区间，并在每个区间使用单独的参数。","text":"1. 维数灾难 一组变量不同的可能配置数量会随着变量的数目的增长而指数级增长，而这种可能的配置数量远大于训练样本的数目。许多传统机器学习算法只是简单的假设在一个新点的输出大致和最接近点的输出相同。 2. 局部不变性和平滑正则化 传统机器学习算法中存在一个使用最广泛的隐式“先验”：平滑先验或称为局部不变性先验。该先验表明学习的函数不应在小区域内发生很大的变化。 许多不同的方法显式或隐式地表示学习函数应具有平滑或局部先验，其旨在鼓励学习过程能学习出函数\\(f^*\\)，对于大多数\\(x\\)和小变动\\(\\epsilon\\)都满足条件: \\[ f^*\\approx f^*(x+\\epsilon) \\] 局部不变方法的一个极端例子是k-最近邻系列算法，当一个区域内的所有点\\(x\\)在训练集中的\\(k\\)个近邻的结果是一样的，则对这些点的预测也是一样的。 大部分核机器也是和附近训练样本相关的训练集的输出进行插值，例如一类重要的核函数是局部核(local kernel)。 决策树也具有平滑学习的局限性，因为它将输入空间分成和叶节点一样多的区间，并在每个区间使用单独的参数。 只要在要学习的真实函数的峰值和谷值处有足够多的样本，那么平滑性假设和相关的无参数学习算法效果都很好。通常当要学习的函数足够平滑，并且只在少数几维变化时，这样的方法一般没有问题。但在高维空间上，即使非常平滑的函数，也会在不同的维度上有不同的变化方式。如果函数在不同的区间中表现的不一样，那么将非常难以用一组训练样本去刻画这个函数。为了解决这个问题，需要额外假设生成数据的分布，依次来建立区域间的依赖关系，那么\\(O(k)\\)个样本是足以描述多如\\(O(2^k)\\)的大量区间。 一般的，机器学习算法往往提出更强的、针对特定问题的假设针例如假设目标函数是周期性的，而由于人工智能任务通常较为复杂，很难限制到简单的、人工指定的性质，因此希望学习算法具有更通用的假设。深度学习的核心思想是假设数据是由因素或特征组合产生，这些因素或特征组合可能来自一个层次结构的多个层级。深度的分布式表示带来的指数增溢有效的解决了维数灾难问题。","categories":[{"name":"Deep Learning Book","slug":"Deep-Learning-Book","permalink":"http://blog.tifosi-m.com/categories/Deep-Learning-Book/"}],"tags":[{"name":"Deep Learning","slug":"Deep-Learning","permalink":"http://blog.tifosi-m.com/tags/Deep-Learning/"}],"keywords":[{"name":"Deep Learning Book","slug":"Deep-Learning-Book","permalink":"http://blog.tifosi-m.com/categories/Deep-Learning-Book/"}]},{"title":"Docker学习笔记:Java Web","slug":"Docker_note_1","date":"2016-07-26T18:30:00.000Z","updated":"2017-09-29T04:58:45.351Z","comments":true,"path":"2016/07/26/Docker_note_1/","link":"","permalink":"http://blog.tifosi-m.com/2016/07/26/Docker_note_1/","excerpt":"因偶然接触到Docker，详细了解了Docker的应用场景觉得Docker的容器化确实是一种未来的趋势难怪自Docker发布以来一直这么火爆，必定在多种场景中都会有所应用，所以我利用空余时间来学下Docker，在博客里记录下学习过程。Docker基本操作这系列文章里就不表述了。 在Java web应用比如利用SSH框架编写的web应用，使用Tomcat部署运行，在这个典型的场景中使用Docker来将其容器化，以便在开发-测试-部署一整套流程中避免遇到环境不一致等很多不必要的麻烦。 以此我们使用3个docker容器来运行Web程序。分别运行tomcat、mysql以及一个数据存储容器专门用来存储数据库数据，将数据库程序与数据存储分离的好处就是但需要升级或者移值数据库时可以更加的方便。 这里我们使用Docker Compose来进行编排，什么是Docker Compose，官方文档描述的很清楚：","text":"因偶然接触到Docker，详细了解了Docker的应用场景觉得Docker的容器化确实是一种未来的趋势难怪自Docker发布以来一直这么火爆，必定在多种场景中都会有所应用，所以我利用空余时间来学下Docker，在博客里记录下学习过程。Docker基本操作这系列文章里就不表述了。 在Java web应用比如利用SSH框架编写的web应用，使用Tomcat部署运行，在这个典型的场景中使用Docker来将其容器化，以便在开发-测试-部署一整套流程中避免遇到环境不一致等很多不必要的麻烦。 以此我们使用3个docker容器来运行Web程序。分别运行tomcat、mysql以及一个数据存储容器专门用来存储数据库数据，将数据库程序与数据存储分离的好处就是但需要升级或者移值数据库时可以更加的方便。 这里我们使用Docker Compose来进行编排，什么是Docker Compose，官方文档描述的很清楚： &gt;Compose is a tool for defining and running multi-container Docker applications. With Compose, you use a Compose file to configure your application’s services. Then, using a single command, you create and start all the services from your configuration. 简单的来说就是利用Docker Compose这一工具可以方便运行多容器的复杂Docker应用，将每个应用定义为服务，使用docker-compose.yml文件进行配置，docker compose将根据这一配置文件来自动构建及运行Docker应用。本文所描述的场景最简单配置如下： 12345678910111213141516171819202122232425version: &apos;2&apos;services: db: image: mysql:latest environment: MYSQL_ROOT_PASSWORD: 12345 MYSQL_DATABASE: es volumes_from: - dbstore web: image: tomcat:8.5 volumes: - &quot;./es/target/es.war:/usr/local/tomcat/webapps/es.war&quot; depends_on: - db ports: - &quot;8888:8080&quot; links: - db dbstore: image: centos:latest volumes: - &quot;/var/lib/mysql&quot; 通过services定义了3个服务，分别是db、web、dbstore，db即mysql数据库，web为tomcat，dbstore用来存储数据库数据,image属性定义了容器所使用的镜像，通过volumes_from定义挂载的Data Container。此外在hibernate的数据库连接url应该使用jdbc:mysql://db:3306/es来配置db为数据库容器主机名，es为具体的数据库，当全部配置完成后即可通过docker-compose up来自动创建以及运行容器。 若需要对数据库文件进行备份可以这样操作： docker run --rm --volumes-from dbstore -v $(pwd):/backup ubuntu tar cvf /backup/backup.tar /var/lib/mysql 将会打包成tar文件，存储在执行命令的当前目录下。 需要恢复数据库则执行： docker run --rm --volumes-from dbstore2 -v $(pwd):/backup ubuntu bash -c &quot;cd /dbdata &amp;&amp; tar xvf /backup/backup.tar --strip 1&quot; Docker Compose的功能很强大，这里只是最简单的使用，Oops!刚接触Docker中，还需要继续摸索 转载请注明出处，谢谢。","categories":[{"name":"Docker","slug":"Docker","permalink":"http://blog.tifosi-m.com/categories/Docker/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"http://blog.tifosi-m.com/tags/Docker/"}],"keywords":[{"name":"Docker","slug":"Docker","permalink":"http://blog.tifosi-m.com/categories/Docker/"}]},{"title":"Hello Maven中央仓库","slug":"Hello Maven Central Repository","date":"2016-07-19T10:30:00.000Z","updated":"2017-09-29T04:58:45.355Z","comments":true,"path":"2016/07/19/Hello Maven Central Repository/","link":"","permalink":"http://blog.tifosi-m.com/2016/07/19/Hello Maven Central Repository/","excerpt":"前段时间做课题相关测试涉及到排序算法，排序算法作为一个程序员必须要求掌握算法，在很多环境下还是蛮有用的，所以就利用点时间整理了下现有常用的排序算法，详见Github sort-tool,包含了冒泡排序、选择排序、归并排序、快速排序、插入排序、希尔排序、堆排序等，有时间打算继续完善下这个工具包。 为了方便以后项目中可能会使用到这些排序算法，避免重复造轮子，就进行了整理和实现，打包成了jar文件方便使用，并将其上传至Maven中央仓库，虽然代码比较简单，但也是我的第一个Maven构建，小小纪念一下。 这里记录下发布至Maven中央仓库的过程。","text":"前段时间做课题相关测试涉及到排序算法，排序算法作为一个程序员必须要求掌握算法，在很多环境下还是蛮有用的，所以就利用点时间整理了下现有常用的排序算法，详见Github sort-tool,包含了冒泡排序、选择排序、归并排序、快速排序、插入排序、希尔排序、堆排序等，有时间打算继续完善下这个工具包。 为了方便以后项目中可能会使用到这些排序算法，避免重复造轮子，就进行了整理和实现，打包成了jar文件方便使用，并将其上传至Maven中央仓库，虽然代码比较简单，但也是我的第一个Maven构建，小小纪念一下。 这里记录下发布至Maven中央仓库的过程。 ## 提交issue Maven 中央仓库一直是由Sonatype公司出资维护的，所以要想提交至中央仓库首先要在该网址上提交一个issue，工作人员会对你进行审核等，其中group id的域名必须是你本人所拥有否则审核不会通过，其实没有自己的域名也没关系，可以使用com.github.你的用户名作为group id.在此注册并提交issue, https://issues.sonatype.org/secure/Dashboard.jspa 等待审批 当工作人员告诉你可以做一次发布时即表示通过审核啦 GPG密钥 Download and install GPG Tool 使用gpg --gen-key命令生成密钥，此过程会让你输入用户名、邮箱、注释信息。 使用gpg --list-keys命令查看生成的密钥信息,形如:pub 2048R/7E97F37D 2016-07-18,pub代表公钥，sub为私钥。 使用gpg --keyserver hkp://pool.sks-keyservers.net --send-keys 7E97F37D将公钥上次至验证服务器，以便其他用户进行校对使用。 使用gpg --keyserver hkp://pool.sks-keyservers.net --recv-keys 7E97F37D可以查看密钥是否上传成功，通常执行上一步上传后需要等待10分钟左右才能查看到。 修改setting.xml Maven的发布方式有很多种详见OSSRH Guide，这里通过Maven方式进行发布所以需要配置setting.xml。setting.xml分为本地设置和全局设置，本地设置只对当前用户有效全局则对所有用户均起左右，本地的配置文件通常在~/m2/文件夹中即本地仓库的目录，全局的setting.xml文件在Maven主程序的conf文件夹中。在setting.xml的&lt;servers&gt; &lt;/servers&gt;标签下添加如下配置: 12345&lt;server&gt; &lt;id&gt;oss&lt;/id&gt; &lt;username&gt;sonatpye注册的用户名&lt;/username&gt; &lt;password&gt;sonatype注册的密码&lt;/password&gt; &lt;/server&gt; 修改pom.xml文件 发布的项目Maven pom.xml的配置必须要求有name description url licenses developers scm这些字段,参考如下: 1234567891011121314151617181920212223242526272829303132&lt;groupId&gt;com.tifosi-m&lt;/groupId&gt;&lt;artifactId&gt;sort-tool&lt;/artifactId&gt;&lt;version&gt;1.0&lt;/version&gt;&lt;packaging&gt;jar&lt;/packaging&gt;&lt;name&gt;sort-tool&lt;/name&gt;&lt;description&gt; This is Sort Algorithm tool,you can easy to use many sort algorithm.&lt;/description&gt;&lt;url&gt;https://github.com/szpssky/sort-tool&lt;/url&gt;&lt;licenses&gt; &lt;license&gt; &lt;name&gt;The Apache Software License, Version 2.0&lt;/name&gt; &lt;url&gt;http://www.apache.org/licenses/LICENSE-2.0.txt&lt;/url&gt; &lt;/license&gt;&lt;/licenses&gt;&lt;developers&gt; &lt;developer&gt; &lt;name&gt;szpssky&lt;/name&gt; &lt;email&gt;szpssky@gmail.com&lt;/email&gt; &lt;/developer&gt;&lt;/developers&gt;&lt;scm&gt; &lt;connection&gt;scm:git@github.com:szpssky/sort-tool.git&lt;/connection&gt; &lt;developerConnection&gt; scm:git@github.com:szpssky/sort-tool.git &lt;/developerConnection&gt; &lt;url&gt;git@github.com:szpssky/sort-tool.git&lt;/url&gt;&lt;/scm&gt; 其次发布jar的话还需要包括source、javadoc、gpg签名，这些可以通过相应maven插件完成，这里的配置可以使用profile，这样当不进行发布时这些过程不会被执行。参考如下: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162&lt;profiles&gt; &lt;profile&gt; &lt;id&gt;release&lt;/id&gt; &lt;distributionManagement&gt; &lt;snapshotRepository&gt; &lt;id&gt;oss&lt;/id&gt; &lt;url&gt;https://oss.sonatype.org/content/repositories/snapshots/&lt;/url&gt; &lt;/snapshotRepository&gt; &lt;repository&gt; &lt;id&gt;oss&lt;/id&gt; &lt;url&gt;https://oss.sonatype.org/service/local/staging/deploy/maven2/&lt;/url&gt; &lt;/repository&gt; &lt;/distributionManagement&gt; &lt;build&gt; &lt;plugins&gt; &lt;!-- Source --&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-source-plugin&lt;/artifactId&gt; &lt;version&gt;3.0.1&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;attach-sources&lt;/id&gt; &lt;goals&gt; &lt;goal&gt;jar-no-fork&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;!-- Javadoc --&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-javadoc-plugin&lt;/artifactId&gt; &lt;version&gt;2.10.3&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;jar&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;!-- Gpg Signature --&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-gpg-plugin&lt;/artifactId&gt; &lt;version&gt;1.6&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;sign-artifacts&lt;/id&gt; &lt;phase&gt;verify&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;sign&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; &lt;/profile&gt; &lt;/profiles&gt; 发布到OSS 一切准备就绪就可以正式发布了，使用mvn clean deploy -P release -Dgpg.passphrase=生成密钥时的密码命令即可自动将要发布的内容上传至服务器上，-P参数代表使用profile，后面的release是使用的profile对应的id，上传成功后登陆OSS，点击左侧staging Repositories后查找你上传的构建,如果没什么问题点击close按钮将其状态由open变为close，此时系统会进行自动检测，如果没有问题状态会变为closed，如果有问题返回修改pom配置重新上传。最后点击release将构建状态从closed变为release。 回到issue 回到之前开的issue中，按要求第一次发布完成后需要告诉工作人员，你已经完成了第一次发布，他们会告诉你在一段时间后自动的同步到Maven中央仓库中并关闭这个issue，至此一次Maven发布完成，此外以后在你申请的group id下发布构建不需要使用issue进行审核了，直接发布即可。 转载请注明出处，谢谢。","categories":[{"name":"Java","slug":"Java","permalink":"http://blog.tifosi-m.com/categories/Java/"}],"tags":[{"name":"Maven","slug":"Maven","permalink":"http://blog.tifosi-m.com/tags/Maven/"}],"keywords":[{"name":"Java","slug":"Java","permalink":"http://blog.tifosi-m.com/categories/Java/"}]},{"title":"Java NIO用allocateDirect()方法创建直接缓存区导致的释放问题","slug":"DirectByteBuffer-release","date":"2016-07-16T20:00:00.000Z","updated":"2017-09-29T04:58:45.351Z","comments":true,"path":"2016/07/16/DirectByteBuffer-release/","link":"","permalink":"http://blog.tifosi-m.com/2016/07/16/DirectByteBuffer-release/","excerpt":"在Java nio中针对缓存区的直接分配有两种方法分别为allocate(int capacity)和allocateDirect(int capacity),区别是前者分配的是堆内内存，后者分配的是堆外内存。当数据处理时操作系统总是将数据读取到系统内存中，再由JVM将数据复制到堆内内存中,所以使用和allocateDirect分配的直接缓存区使得I/O效率大大高于分配间接缓存区,但是使用直接缓存区在分配和销毁时代价比堆内存要大，所以在使用上应具体分析。以下是Sun文档对直接缓存区的描述: 给定一个直接字节缓冲区，Java 虚拟机将尽最大努力直接对它执行本机 I/O 操作。也就是说，它会在每一次调用底层操作系统的本机 I/O 操作之前(或之后)，尝试避免将缓冲区的内容拷贝到一个中间缓冲区中(或者从一个中间缓冲区中拷贝数据)。","text":"在Java nio中针对缓存区的直接分配有两种方法分别为allocate(int capacity)和allocateDirect(int capacity),区别是前者分配的是堆内内存，后者分配的是堆外内存。当数据处理时操作系统总是将数据读取到系统内存中，再由JVM将数据复制到堆内内存中,所以使用和allocateDirect分配的直接缓存区使得I/O效率大大高于分配间接缓存区,但是使用直接缓存区在分配和销毁时代价比堆内存要大，所以在使用上应具体分析。以下是Sun文档对直接缓存区的描述: 给定一个直接字节缓冲区，Java 虚拟机将尽最大努力直接对它执行本机 I/O 操作。也就是说，它会在每一次调用底层操作系统的本机 I/O 操作之前(或之后)，尝试避免将缓冲区的内容拷贝到一个中间缓冲区中(或者从一个中间缓冲区中拷贝数据)。 正因为这种特性，使用直接缓存区时会存在堆外内存释放的问题，因为是堆外内存所以不会在普通GC（或Minor GC）阶段进行垃圾回收,只有在Full GC时才会回收这部分内存，这就导致一个问题可能会造成堆外内存溢出异常：当虚拟机运行时如果给JVM充足的堆空间，因为堆空间充足所以并不会触发Full GC来进行垃圾回收，当程序不断申请堆外内存时，系统的本地内存将越来少而此时使用完成的缓存区又得不到回收，最终将导致OutofMemoryError:Direct buffer memory异常。 在Java中缓冲区对象Buffer并没有直接方法可以手动销毁缓存区,而查看Java源代码DirectBuffer接口可以看到： 12345678910package sun.nio.ch;import sun.misc.Cleaner;public interface DirectBuffer &#123; long address(); Object attachment(); Cleaner cleaner();&#125; 此接口中存在一个Cleaner对象，再来看ByteBuffer具体实现类DirectByteBuffer中的构造方法： 123456789101112131415161718192021222324DirectByteBuffer(int cap) &#123; super(-1, 0, cap, cap); boolean pa = VM.isDirectMemoryPageAligned(); int ps = Bits.pageSize(); long size = Math.max(1L, (long)cap + (pa ? ps : 0)); Bits.reserveMemory(size, cap); long base = 0; try &#123; base = unsafe.allocateMemory(size); &#125; catch (OutOfMemoryError x) &#123; Bits.unreserveMemory(size, cap); throw x; &#125; unsafe.setMemory(base, size, (byte) 0); if (pa &amp;&amp; (base % ps != 0)) &#123; // Round up to page boundary address = base + ps - (base &amp; (ps - 1)); &#125; else &#123; address = base; &#125; cleaner = Cleaner.create(this, new Deallocator(base, size, cap)); att = null; &#125; 当创建一个DirectByteBuffer时将会产生创建一个Cleaner对象,我们通过调用该对象的clean()方法即可以释放通过allocateDirect(int capacity)分配的堆外内存。 12345if (rBuffer == null) return;Cleaner cleaner = ((DirectBuffer) rBuffer).cleaner();if (cleaner != null) cleaner.clean(); 转载请注明出处，谢谢。","categories":[{"name":"Java NIO","slug":"Java-NIO","permalink":"http://blog.tifosi-m.com/categories/Java-NIO/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://blog.tifosi-m.com/tags/Java/"}],"keywords":[{"name":"Java NIO","slug":"Java-NIO","permalink":"http://blog.tifosi-m.com/categories/Java-NIO/"}]},{"title":"Hexo-Travis CI","slug":"Hexo-Travis","date":"2016-07-09T22:30:00.000Z","updated":"2017-09-29T04:58:45.355Z","comments":true,"path":"2016/07/09/Hexo-Travis/","link":"","permalink":"http://blog.tifosi-m.com/2016/07/09/Hexo-Travis/","excerpt":"Travis CI is a hosted, distributed continuous integration service used to build and test software projects hosted at GitHub. Hexo blog system could using Travis CI.It will help you automatic building and Deploying. How to using Travis CI for Hexo","text":"Travis CI is a hosted, distributed continuous integration service used to build and test software projects hosted at GitHub. Hexo blog system could using Travis CI.It will help you automatic building and Deploying. How to using Travis CI for Hexo Preparation Create travis.yml 123456789101112language: node_jssudo: falsenode_js: - \"4\"cache: npm: ture directories: - node_modules - themes/tranquilpeak/node_modulesbranches: only: - blog-source Turn on the repository switch in the Travis. Setp 1 Generating a new SSH key. 1ssh-keygen -t rsa -b 4096 -C \"your_email@example.com\" It will generate two files, ssh_key and ssh_key.pub. Copies the contents of the ssh_key.pub file to your clipboard and adding it to the Repository`s Deploy Keys. #### Setp 2 Encryption private key. Install Travis CI Command line client 1gem install travis Login in travis 1travis login --auto Encrypted ssh_key 1travis encrypt-file id_rsa --add It will generate ssh_key.enc file and automatic write some config into .travis.yml. Modify .travis.yml file 12- openssl aes-256-cbc -K $encrypted_*****_key -iv $encrypted_*****_iv -in ssh_key.enc -out ~/.ssh/id_rsa -d Setp 3 It will make building systems to establish a default ssh connection Create ssh_config file and modify .travis.yml 12345Host github.com User git StrictHostKeyChecking no IdentityFile ~/.ssh/id_rsa IdentitiesOnly yes 1234- chmod 600 ~/.ssh/id_rsa- eval $(ssh-agent)- ssh-add ~/.ssh/id_rsa- cp ssh_config ~/.ssh/config Setp 4 Travis Full configuration. 12345678910111213141516171819202122232425262728293031323334language: node_jssudo: falsenode_js: - \"4\"cache: npm: ture directories: - node_modulesbranches: only: - blog-sourcebefore_install:- openssl aes-256-cbc -K $encrypted_*****_key -iv $encrypted_*****_iv -in ssh_key.enc -out ~/.ssh/id_rsa -d- chmod 600 ~/.ssh/id_rsa- eval $(ssh-agent)- ssh-add ~/.ssh/id_rsa- cp ssh_config ~/.ssh/config- git config --global user.name \"yourname\"- git config --global user.email \"your_email@example.com\"install:- npm install -g hexo-cli- npm install- npm install hexo-generator-sitemap --save- npm install hexo-generator-baidu-sitemap --savescript:- hexo cl- hexo g- hexo d Finally Commit and push to the repository.Enjoy It!","categories":[{"name":"CI","slug":"CI","permalink":"http://blog.tifosi-m.com/categories/CI/"}],"tags":[{"name":"Travis CI","slug":"Travis-CI","permalink":"http://blog.tifosi-m.com/tags/Travis-CI/"}],"keywords":[{"name":"CI","slug":"CI","permalink":"http://blog.tifosi-m.com/categories/CI/"}]},{"title":"GitFlow工作流","slug":"GitFlow","date":"2016-07-07T22:21:54.000Z","updated":"2017-09-29T04:58:45.351Z","comments":true,"path":"2016/07/07/GitFlow/","link":"","permalink":"http://blog.tifosi-m.com/2016/07/07/GitFlow/","excerpt":"最近没事喜欢逛逛Github，看看热门的开源项目，感觉还是蛮不错的，可以了解到最新的前沿技术，拓展知识面。GitHub之于开源世界如同FaceBook之与互联网社交一般。今天就来分享一下针对Git分布式版本控制系统，在软件开发中所使用的GitFlow工作流。 工作流有各种各样，主要可分为集中式工作流、功能分支工作流、GitFlow工作流、Forking工作流。其中GitFlow工作流可以说是功能分支工作流的升华。 何为GitFlow Gitflow工作流通过为功能开发、发布准备和维护分配独立的分支，让发布迭代过程更流畅。严格的分支模型也为大型项目提供了一些非常必要的结构。","text":"最近没事喜欢逛逛Github，看看热门的开源项目，感觉还是蛮不错的，可以了解到最新的前沿技术，拓展知识面。GitHub之于开源世界如同FaceBook之与互联网社交一般。今天就来分享一下针对Git分布式版本控制系统，在软件开发中所使用的GitFlow工作流。 工作流有各种各样，主要可分为集中式工作流、功能分支工作流、GitFlow工作流、Forking工作流。其中GitFlow工作流可以说是功能分支工作流的升华。 何为GitFlow Gitflow工作流通过为功能开发、发布准备和维护分配独立的分支，让发布迭代过程更流畅。严格的分支模型也为大型项目提供了一些非常必要的结构。 Gitflow工作流定义了一个围绕项目发布的严格分支模型，该工作流没有超出分支工作流的概念，但明确的给分支定义了一个角色，并且定义分支与分支之间的交互，如下这张完整的Gitflow工作流示意图。 这张图中有5个分支，分别是master、develop、feature、release、hotfixes. ### 历史分支 相对使用仅有的一个master分支，Gitflow分支使用2个分支记录项目的历史，并且建议在任何一个项目开发中无论使用怎样的工作流都应该使用至少1个辅助分支，而不是仅有一个master分支。 develop分支作为功能的集成分支，master作为项目的主分支，项目的整体进度应该是沿着master分支一直向前开发，通常在master分支上使用tag来标记开发的版本号。其余分支都是围绕着这两个分支来进行 ### 功能分支 每一个需要开发的新功能，在开发时都应该建立相应的功能分支并且各个功能分支不是从master分支上分出，而是应该在develop分支的基础上分出功能分支，当功能完成是应合并回develop分支。 ### 发布分支 所谓发布分支是指，当项目完成到一定进度可以发布一个版本时所使用的分支，发布分支也是从develop分支上分出，新建立的这个release分支，作为发布准备所使用，在此时间点之后项目所做的修改也不能再加的这个分支上去了，此分支只可作为为当前发布进行bug修复、文档储存等说使用，如果发布工作已经全部完成则把发布分支合并到master分支并给master打上tag作为一个版本进行发布。当然在release分支建立之后所做的修改此时也应该合并到develop分支中。 发布分支的作用就是在准备版本发布时，也不影响开发的继续进行。 ### 维护分支 维护分支，在这张工作流图示中即hotfixes分支，该分支仅作为给发布版本进行bug修复使用，这个分支从master分支上分出，修复完成便立即合并回master和develop，并且给master打上新tag分配一个新版本号。 此即为GitFlow工作流的主要内容，在实际开发中，可以结合Pull Request机制进行代码评审(code review)，提高代码质量。 针对Code Review分享一个Google的编程规范Google Style Guides 转载请注明出处，谢谢。","categories":[{"name":"Git","slug":"Git","permalink":"http://blog.tifosi-m.com/categories/Git/"}],"tags":[{"name":"软件工程","slug":"软件工程","permalink":"http://blog.tifosi-m.com/tags/软件工程/"}],"keywords":[{"name":"Git","slug":"Git","permalink":"http://blog.tifosi-m.com/categories/Git/"}]},{"title":"【转载】软件设计原则","slug":"Software design principles","date":"2016-06-24T18:35:27.000Z","updated":"2017-09-29T04:58:45.355Z","comments":true,"path":"2016/06/24/Software design principles/","link":"","permalink":"http://blog.tifosi-m.com/2016/06/24/Software design principles/","excerpt":"Don’t Repeat Yourself (DRY) DRY 是一个最简单的法则，也是最容易被理解的。但它也可能是最难被应用的（因为要做到这样，我们需要在泛型设计上做相当的努力，这并不是一件容易的事）。它意味着，当我们在两个或多个地方的时候发现一些相似的代码的时候，我们需要把他们的共性抽象出来形一个唯一的新方法，并且改变现有的地方的代码让他们以一些合适的参数调用这个新的方法。 参考：http://en.wikipedia.org/wiki/Don%27t_repeat_yourself Keep It Simple, Stupid (KISS) KISS原则在设计上可能最被推崇的，在家装设计，界面设计 ，操作设计上，复杂的东西越来越被众人所BS了，而简单的东西越来越被人所认可，比如这些UI的设计和我们中国网页（尤其是新浪的网页）者是负面的例子。“宜家”（IKEA）简约、效率的家居设计、生产思路；“微软”（Microsoft）“所见即所得”的理念；“谷歌”（Google)简约、直接的商业风格，无一例外的遵循了“kiss”原则，也正是“kiss”原则，成就了这些看似神奇的商业经典。而苹果公司的iPhone/iPad将这个原则实践到了极至。 把一个事情搞复杂是一件简单的事，但要把一个复杂的事变简单，这是一件复杂的事。","text":"Don’t Repeat Yourself (DRY) DRY 是一个最简单的法则，也是最容易被理解的。但它也可能是最难被应用的（因为要做到这样，我们需要在泛型设计上做相当的努力，这并不是一件容易的事）。它意味着，当我们在两个或多个地方的时候发现一些相似的代码的时候，我们需要把他们的共性抽象出来形一个唯一的新方法，并且改变现有的地方的代码让他们以一些合适的参数调用这个新的方法。 参考：http://en.wikipedia.org/wiki/Don%27t_repeat_yourself Keep It Simple, Stupid (KISS) KISS原则在设计上可能最被推崇的，在家装设计，界面设计 ，操作设计上，复杂的东西越来越被众人所BS了，而简单的东西越来越被人所认可，比如这些UI的设计和我们中国网页（尤其是新浪的网页）者是负面的例子。“宜家”（IKEA）简约、效率的家居设计、生产思路；“微软”（Microsoft）“所见即所得”的理念；“谷歌”（Google)简约、直接的商业风格，无一例外的遵循了“kiss”原则，也正是“kiss”原则，成就了这些看似神奇的商业经典。而苹果公司的iPhone/iPad将这个原则实践到了极至。 把一个事情搞复杂是一件简单的事，但要把一个复杂的事变简单，这是一件复杂的事。 参考：http://en.wikipedia.org/wiki/KISS_principle Program to an interface, not an implementation 这是设计模式中最根本的哲学，注重接口，而不是实现，依赖接口，而不是实现。接口是抽象是稳定的，实现则是多种多样的。以后面我们会面向对象的SOLID原则中会提到我们的依赖倒置原则，就是这个原则的的另一种样子。还有一条原则叫 Composition over inheritance（喜欢组合而不是继承），这两条是那23个经典设计模式中的设计原则。 Command-Query Separation (CQS) – 命令-查询分离原则 查询：当一个方法返回一个值来回应一个问题的时候，它就具有查询的性质； 命令：当一个方法要改变对象的状态的时候，它就具有命令的性质； 通常，一个方法可能是纯的Command模式或者是纯的Query模式，或者是两者的混合体。在设计接口时，如果可能，应该尽量使接口单一化，保证方法的行为严格的是命令或者是查询，这样查询方法不会改变对象的状态，没有副作用，而会改变对象的状态的方法不可能有返回值。也就是说：如果我们要问一个问题，那么就不应该影响到它的答案。实际应用，要视具体情况而定，语义的清晰性和使用的简单性之间需要权衡。将Command和Query功能合并入一个方法，方便了客户的使用，但是，降低了清晰性，而且，可能不便于基于断言的程序设计并且需要一个变量来保存查询结果。 在系统设计中，很多系统也是以这样原则设计的，查询的功能和命令功能的系统分离，这样有则于系统性能，也有利于系统的安全性。 参考：http://en.wikipedia.org/wiki/Command-query_separation You Ain’t Gonna Need It (YAGNI) 这个原则简而言之为——只考虑和设计必须的功能，避免过度设计。只实现目前需要的功能，在以后您需要更多功能时，可以再进行添加。 如无必要，勿增复杂性。 软件开发先是一场沟通博弈。 以前本站有一篇关于过度重构的文章，这个示例就是这个原则的反例。而，WebSphere的设计者就表示过他过度设计了这个产品。我们的程序员或是架构师在设计系统的时候，会考虑很多扩展性的东西，导致在架构与设计方面使用了大量折衷，最后导致项目失败。这是个令人感到讽刺的教训，因为本来希望尽可能延长项目的生命周期，结果反而缩短了生命周期。 参考：http://en.wikipedia.org/wiki/You_Ain%27t_Gonna_Need_It Law of Demeter – 迪米特法则 迪米特法则(Law of Demeter)，又称“最少知识原则”（Principle of Least Knowledge），其来源于1987年荷兰大学的一个叫做Demeter的项目。Craig Larman把Law of Demeter又称作“不要和陌生人说话”。在《程序员修炼之道》中讲LoD的那一章叫作“解耦合与迪米特法则”。关于迪米特法则有一些很形象的比喻： 如果你想让你的狗跑的话，你会对狗狗说还是对四条狗腿说？ 如果你去店里买东西，你会把钱交给店员，还是会把钱包交给店员让他自己拿？ 和狗的四肢说话？让店员自己从钱包里拿钱？这听起来有点荒唐，不过在我们的代码里这几乎是见怪不怪的事情了。 对于LoD，正式的表述如下： 对于对象 ‘O’ 中一个方法’M’，M应该只能够访问以下对象中的方法： 对象O； 与O直接相关的Component Object； 由方法M创建或者实例化的对象； 作为方法M的参数的对象。 在《Clean Code》一书中，有一段Apache framework中的一段违反了LoD的代码： final String outputDir = ctxt.getOptions().getScratchDir().getAbsolutePath(); 这么长的一串对其它对象的细节，以及细节的细节，细节的细节的细节……的调用，增加了耦合，使得代码结构复杂、僵化，难以扩展和维护。 在《重构》一书中的代码的环味道中有一种叫做“Feature Envy”(依恋情结），形象的描述了一种违反了LoC的情况。Feature Envy就是说一个对象对其它对象的内容更有兴趣，也就是说老是羡慕别的对象的成员、结构或者功能，大老远的调用人家的东西。这样的结构显然是不合理的。我们的程序应该写得比较“害羞”。不能像前面例子中的那个不把自己当外人的店员一样，拿过客人的钱包自己把钱拿出来。“害羞”的程序只和自己最近的朋友交谈。这种情况下应该调整程序的结构，让那个对象自己拥有它羡慕的feature，或者使用合理的设计模式（例如Facade和Mediator）。 参考：http://en.wikipedia.org/wiki/Principle_of_Least_Knowledge 面向对象的S.O.L.I.D 原则 一般来说这是面向对象的五大设计原则，但是，我觉得这些原则可适用于所有的软件开发。 Single Responsibility Principle (SRP) – 职责单一原则 关于单一职责原则，其核心的思想是：一个类，只做一件事，并把这件事做好，其只有一个引起它变化的原因。单一职责原则可以看作是低耦合、高内聚在面向对象原则上的引申，将职责定义为引起变化的原因，以提高内聚性来减少引起变化的原因。职责过多，可能引起它变化的原因就越多，这将导致职责依赖，相互之间就产生影响，从而极大的损伤其内聚性和耦合度。单一职责，通常意味着单一的功能，因此不要为一个模块实现过多的功能点，以保证实体只有一个引起它变化的原因。 Unix/Linux是这一原则的完美体现者。各个程序都独立负责一个单一的事。 Windows是这一原则的反面示例。几乎所有的程序都交织耦合在一起。 Open/Closed Principle (OCP) – 开闭原则 关于开发封闭原则，其核心的思想是：模块是可扩展的，而不可修改的。也就是说，对扩展是开放的，而对修改是封闭的。 对扩展开放，意味着有新的需求或变化时，可以对现有代码进行扩展，以适应新的情况。 对修改封闭，意味着类一旦设计完成，就可以独立完成其工作，而不要对类进行任何修改。 对于面向对象来说，需要你依赖抽象，而不是实现，23个经典设计模式中的“策略模式”就是这个实现。对于非面向对象编程，一些API需要你传入一个你可以扩展的函数，比如我们的C 语言的qsort()允许你提供一个“比较器”，STL中的容器类的内存分配，ACE中的多线程的各种锁。对于软件方面，浏览器的各种插件属于这个原则的实践。 Liskov substitution principle (LSP) – 里氏代换原则 软件工程大师Robert C. Martin把里氏代换原则最终简化为一句话：“Subtypes must be substitutable for their base types”。也就是，子类必须能够替换成它们的基类。即：子类应该可以替换任何基类能够出现的地方，并且经过替换以后，代码还能正常工作。另外，不应该在代码中出现if/else之类对子类类型进行判断的条件。里氏替换原则LSP是使代码符合开闭原则的一个重要保证。正是由于子类型的可替换性才使得父类型的模块在无需修改的情况下就可以扩展。 这么说来，似乎有点教条化，我非常建议大家看看这个原则个两个最经典的案例——“正方形不是长方形”和“鸵鸟不是鸟”。通过这两个案例，你会明白《墨子 小取》中说的 ——“娣，美人也，爱娣，非爱美人也….盗，人也；恶盗，非恶人也。”——妹妹虽然是美人，但喜欢妹妹并不代表喜欢美人。盗贼是人，但讨厌盗贼也并不代表就讨厌人类。这个原则让你考虑的不是语义上对象的间的关系，而是实际需求的环境。 在很多情况下，在设计初期我们类之间的关系不是很明确，LSP则给了我们一个判断和设计类之间关系的基准：需不需要继承，以及怎样设计继承关系。 Interface Segregation Principle (ISP) – 接口隔离原则 接口隔离原则意思是把功能实现在接口中，而不是类中，使用多个专门的接口比使用单一的总接口要好。 举个例子，我们对电脑有不同的使用方式，比如：写作，通讯，看电影，打游戏，上网，编程，计算，数据等，如果我们把这些功能都声明在电脑的抽类里面，那么，我们的上网本，PC机，服务器，笔记本的实现类都要实现所有的这些接口，这就显得太复杂了。所以，我们可以把其这些功能接口隔离开来，比如：工作学习接口，编程开发接口，上网娱乐接口，计算和数据服务接口，这样，我们的不同功能的电脑就可以有所选择地继承这些接口。 这个原则可以提升我们“搭积木式”的软件开发。对于设计来说，Java中的各种Event Listener和Adapter，对于软件开发来说，不同的用户权限有不同的功能，不同的版本有不同的功能，都是这个原则的应用。 Dependency Inversion Principle (DIP) – 依赖倒置原则 高层模块不应该依赖于低层模块的实现，而是依赖于高层抽象。 举个例子，墙面的开关不应该依赖于电灯的开关实现，而是应该依赖于一个抽象的开关的标准接口，这样，当我们扩展程序的时候，我们的开关同样可以控制其它不同的灯，甚至不同的电器。也就是说，电灯和其它电器继承并实现我们的标准开关接口，而我们的开关产商就可不需要关于其要控制什么样的设备，只需要关心那个标准的开关标准。这就是依赖倒置原则。 这就好像浏览器并不依赖于后面的web服务器，其只依赖于HTTP协议。这个原则实在是太重要了，社会的分工化，标准化都是这个设计原则的体现。 参考：http://en.wikipedia.org/wiki/Solid_(object-oriented_design) Common Closure Principle（CCP）– 共同封闭原则 一个包中所有的类应该对同一种类型的变化关闭。一个变化影响一个包，便影响了包中所有的类。一个更简短的说法是：一起修改的类，应该组合在一起（同一个包里）。如果必须修改应用程序里的代码，我们希望所有的修改都发生在一个包里（修改关闭），而不是遍布在很多包里。CCP原则就是把因为某个同样的原因而需要修改的所有类组合进一个包里。如果2个类从物理上或者从概念上联系得非常紧密，它们通常一起发生改变，那么它们应该属于同一个包。 CCP延伸了开闭原则（OCP）的“关闭”概念，当因为某个原因需要修改时，把需要修改的范围限制在一个最小范围内的包里。 参考：http://c2.com/cgi/wiki?CommonClosurePrinciple Common Reuse Principle (CRP) – 共同重用原则 包的所有类被一起重用。如果你重用了其中的一个类，就重用全部。换个说法是，没有被一起重用的类不应该被组合在一起。CRP原则帮助我们决定哪些类应该被放到同一个包里。依赖一个包就是依赖这个包所包含的一切。当一个包发生了改变，并发布新的版本，使用这个包的所有用户都必须在新的包环境下验证他们的工作，即使被他们使用的部分没有发生任何改变。因为如果包中包含有未被使用的类，即使用户不关心该类是否改变，但用户还是不得不升级该包并对原来的功能加以重新测试。 CCP则让系统的维护者受益。CCP让包尽可能大（CCP原则加入功能相关的类），CRP则让包尽可能小（CRP原则剔除不使用的类）。它们的出发点不一样，但不相互冲突。 参考：http://c2.com/cgi/wiki?CommonReusePrinciple Hollywood Principle – 好莱坞原则 好莱坞原则就是一句话——“don’t call us, we’ll call you.”。意思是，好莱坞的经纪人们不希望你去联系他们，而是他们会在需要的时候来联系你。也就是说，所有的组件都是被动的，所有的组件初始化和调用都由容器负责。组件处在一个容器当中，由容器负责管理。 简单的来讲，就是由容器控制程序之间的关系，而非传统实现中，由程序代码直接操控。这也就是所谓“控制反转”的概念所在： 不创建对象，而是描述创建对象的方式。 在代码中，对象与服务没有直接联系，而是容器负责将这些联系在一起。 控制权由应用代码中转到了外部容器，控制权的转移，是所谓反转。 好莱坞原则就是IoC（Inversion of Control）或DI（Dependency Injection ）的基础原则。这个原则很像依赖倒置原则，依赖接口，而不是实例，但是这个原则要解决的是怎么把这个实例传入调用类中？你可能把其声明成成员，你可以通过构造函数，你可以通过函数参数。但是 IoC可以让你通过配置文件，一个由Service Container 读取的配置文件来产生实际配置的类。但是程序也有可能变得不易读了，程序的性能也有可能还会下降。 参考： http://en.wikipedia.org/wiki/Hollywood_Principle http://en.wikipedia.org/wiki/Inversion_of_Control High Cohesion &amp; Low/Loose coupling &amp; – 高内聚， 低耦合 这个原则是UNIX操作系统设计的经典原则，把模块间的耦合降到最低，而努力让一个模块做到精益求精。 内聚：一个模块内各个元素彼此结合的紧密程度 耦合：一个软件结构内不同模块之间互连程度的度量 内聚意味着重用和独立，耦合意味着多米诺效应牵一发动全身。 参考： http://en.wikipedia.org/wiki/Coupling_(computer_science) http://en.wikipedia.org/wiki/Cohesion_(computer_science) Convention over Configuration（CoC）– 惯例优于配置原则 简单点说，就是将一些公认的配置方式和信息作为内部缺省的规则来使用。例如，Hibernate的映射文件，如果约定字段名和类属性一致的话，基本上就可以不要这个配置文件了。你的应用只需要指定不convention的信息即可，从而减少了大量convention而又不得不花时间和精力啰里啰嗦的东东。配置文件很多时候相当的影响开发效率。 Rails 中很少有配置文件（但不是没有，数据库连接就是一个配置文件），Rails 的fans号称期开发效率是 java 开发的 10 倍，估计就是这个原因。Maven也使用了CoC原则，当你执行mvn -compile命令的时候，不需要指源文件放在什么地方，而编译以后的class文件放置在什么地方也没有指定，这就是CoC原则。 参考：http://en.wikipedia.org/wiki/Convention_over_Configuration Separation of Concerns (SoC) – 关注点分离 SoC 是计算机科学中最重要的努力目标之一。这个原则，就是在软件开发中，通过各种手段，将问题的各个关注点分开。如果一个问题能分解为独立且较小的问题，就是相对较易解决的。问题太过于复杂，要解决问题需要关注的点太多，而程序员的能力是有限的，不能同时关注于问题的各个方面。正如程序员的记忆力相对于计算机知识来说那么有限一样，程序员解决问题的能力相对于要解决的问题的复杂性也是一样的非常有限。在我们分析问题的时候，如果我们把所有的东西混在一起讨论，那么就只会有一个结果——乱。 我记得在上一家公司有一个项目，讨论就讨论了1年多，项目本来不复杂，但是没有使用SoC，全部的东西混为一谈，再加上一堆程序员注入了各种不同的观点和想法，整个项目一下子就失控了。最后，本来一个1年的项目做了3年。 实现关注点分离的方法主要有两种，一种是标准化，另一种是抽象与包装。标准化就是制定一套标准，让使用者都遵守它，将人们的行为统一起来，这样使用标准的人就不用担心别人会有很多种不同的实现，使自己的程序不能和别人的配合。Java EE就是一个标准的大集合。每个开发者只需要关注于标准本身和他所在做的事情就行了。就像是开发镙丝钉的人只专注于开发镙丝钉就行了，而不用关注镙帽是怎么生产的，反正镙帽和镙丝钉按标来就一定能合得上。不断地把程序的某些部分抽像差包装起来，也是实现关注点分离的好方法。一旦一个函数被抽像出来并实现了，那么使用函数的人就不用关心这个函数是如何实现的，同样的，一旦一个类被抽像并实现了，类的使用者也不用再关注于这个类的内部是如何实现的。诸如组件，分层，面向服务，等等这些概念都是在不同的层次上做抽像和包装，以使得使用者不用关心它的内部实现细节。 说白了还是“高内聚，低耦合”。 参考：http://sulong.me/archives/99 Design by Contract (DbC) – 契约式设计 DbC的核心思想是对软件系统中的元素之间相互合作以及“责任”与“义务”的比喻。这种比喻从商业活动中“客户”与“供应商”达成“契约”而得来。例如： 供应商必须提供某种产品（责任），并且他有权期望客户已经付款（权利）。 客户必须付款（责任），并且有权得到产品（权利）。 契约双方必须履行那些对所有契约都有效的责任，如法律和规定等。 同样的，如果在程序设计中一个模块提供了某种功能，那么它要： 期望所有调用它的客户模块都保证一定的进入条件：这就是模块的先验条件（客户的义务和供应商的权利，这样它就不用去处理不满足先验条件的情况）。 保证退出时给出特定的属性：这就是模块的后验条件——（供应商的义务，显然也是客户的权利）。 在进入时假定，并在退出时保持一些特定的属性：不变式。 契约就是这些权利和义务的正式形式。我们可以用“三个问题”来总结DbC，并且作为设计者要经常问： 它期望的是什么？ 它要保证的是什么？ 它要保持的是什么？ 根据Bertrand Meyer氏提出的DBC概念的描述，对于类的一个方法，都有一个前提条件以及一个后续条件，前提条件说明方法接受什么样的参数数据等，只有前提条件得到满足时，这个方法才能被调用；同时后续条件用来说明这个方法完成时的状态，如果一个方法的执行会导致这个方法的后续条件不成立，那么这个方法也不应该正常返回。 现在把前提条件以及后续条件应用到继承子类中，子类方法应该满足： 前提条件不强于基类． 后续条件不弱于基类． 换句话说，通过基类的接口调用一个对象时，用户只知道基类前提条件以及后续条件。因此继承类不得要求用户提供比基类方法要求的更强的前提条件，亦即，继承类方法必须接受任何基类方法能接受的任何条件（参数）。同样，继承类必须顺从基类的所有后续条件，亦即，继承类方法的行为和输出不得违反由基类建立起来的任何约束，不能让用户对继承类方法的输出感到困惑。 这样，我们就有了基于契约的LSP，基于契约的LSP是LSP的一种强化。 参考：http://en.wikipedia.org/wiki/Design_by_contract Acyclic Dependencies Principle (ADP) – 无环依赖原则 包之间的依赖结构必须是一个直接的无环图形，也就是说，在依赖结构中不允许出现环（循环依赖）。如果包的依赖形成了环状结构，怎么样打破这种循环依赖呢？有2种方法可以打破这种循环依赖关系：第一种方法是创建新的包，如果A、B、C形成环路依赖，那么把这些共同类抽出来放在一个新的包D里。这样就把C依赖A变成了C依赖D以及A依赖D，从而打破了循环依赖关系。第二种方法是使用DIP（依赖倒置原则）和ISP（接口分隔原则）设计原则。 无环依赖原则（ADP）为我们解决包之间的关系耦合问题。在设计模块时，不能有循环依赖。 参考：http://c2.com/cgi/wiki?AcyclicDependenciesPrinciple 转载自:coolshell","categories":[{"name":"转载","slug":"转载","permalink":"http://blog.tifosi-m.com/categories/转载/"}],"tags":[{"name":"软件工程","slug":"软件工程","permalink":"http://blog.tifosi-m.com/tags/软件工程/"},{"name":"设计原则","slug":"设计原则","permalink":"http://blog.tifosi-m.com/tags/设计原则/"}],"keywords":[{"name":"转载","slug":"转载","permalink":"http://blog.tifosi-m.com/categories/转载/"}]},{"title":"Java NIO初探","slug":"Java-NIO","date":"2016-06-22T16:08:27.000Z","updated":"2017-09-29T04:58:45.355Z","comments":true,"path":"2016/06/22/Java-NIO/","link":"","permalink":"http://blog.tifosi-m.com/2016/06/22/Java-NIO/","excerpt":"因为研究项目实验论证方面的需要所以接触到了Java NIO,之前因为使用普通的Java IO导致性能十分低下，实验数据相当不理想。至此了解到了Java NIO技术。 &gt;新的输入/输出 (NIO) 库是在 JDK 1.4 中引入的。NIO 弥补了原来的 I/O 的不足，它在标准Java代码中提供了高速的、面向块的I/O。通过定义包含数据的类，以及通过以块的形式处理这些数据，NIO不用使用本机代码就可以利用低级优化，这是原来的 I/O 包所无法做到的。","text":"因为研究项目实验论证方面的需要所以接触到了Java NIO,之前因为使用普通的Java IO导致性能十分低下，实验数据相当不理想。至此了解到了Java NIO技术。 &gt;新的输入/输出 (NIO) 库是在 JDK 1.4 中引入的。NIO 弥补了原来的 I/O 的不足，它在标准Java代码中提供了高速的、面向块的I/O。通过定义包含数据的类，以及通过以块的形式处理这些数据，NIO不用使用本机代码就可以利用低级优化，这是原来的 I/O 包所无法做到的。 NIO由以下几个核心部分组成： - Channels - Buffers - Selectors Java IO和Java NIO的区别 IO NIO 面向流 面向缓存 阻塞 非阻塞 无 选择器 面向流与面向缓存 所谓面向流即读取时每次从流读取一个或多个字符直到所有字符全部读取完毕，你不能前后移动的从流中读取某一数据，而Java NIO则是面向缓存，所有数据读取到一个缓存区中，你可以在缓存区中任意移动来读取所在位置的数据，使得该方法对数据的处理更加灵活。 ### 阻塞与非阻塞 阻塞型IO，每当进行读取或写入操作的时候该线程被阻塞直到读取或写入完成，在此期间当前线程将不能做其他事情了，只能等待完成，而非阻塞型IO，到发出读或写指令的时候，当前线程可以执行其他命令，而不会被阻塞。 选择器 Java NIO的选择器允许一个单独的线程来监视多个通道。 ## Java NIO使用 1234567891011121314151617181920/*** 写文件*/File srcFile = new File(\"/root/spill_out/\" + count + \".txt\");RandomAccessFile raf = new RandomAccessFile(srcFile, \"rw\");FileChannel fileChannel = raf.getChannel();ByteBuffer rBuffer = ByteBuffer.allocate(32 * 1024 * 1024);try &#123; int size = mappedKeyValue.size(); for (int i = 0; i &lt; size; i++) &#123; KeyValue&lt;String, Integer&gt; keyValue = mappedKeyValue.remove(0); rBuffer.put((keyValue.getKey().toString() + \" \" + keyValue.getValue().toString() + \"\\n\").getBytes()); &#125;&#125;catch (Exception e) &#123; e.printStackTrace();&#125;rBuffer.flip();fileChannel.write(rBuffer);fileChannel.close();raf.close(); 该代码是一个写文件操作，通道FileChannel的实例只有唯一的方法可以获取，那就是调用.getChannel()方法，ByteBuffer为字节行缓存区，需要手动分配缓存区大小，使用put()方法往缓存区中写入数据。 1234567891011121314151617181920212223/*** 读文件*/RandomAccessFile raf = new RandomAccessFile(new File(filename), \"r\");FileChannel fc = raf.getChannel();MappedByteBuffer mbb = fc.map(FileChannel.MapMode.READ_ONLY, 0, fc.size());StringBuffer sbf = new StringBuffer();while(mbb.remaining()&gt;0)&#123; char data = (char)mbb.get(); if(data!='\\n')&#123; sbf.append(data); &#125;else&#123; wcMR.addKeyValue(0,sbf.toString()); sbf.setLength(0); if(count == 800000)&#123; wcMR.startMap(); count=0; &#125; count++; &#125;&#125;fc.close();raf.close(); 这里为了提高超大文件读取的性能采用MappedByteBuffer，即内存映射文件，通过map()方法将文件映射成内存映射文件，通过调用get()方法获取数据，该示例采用按行读取数据。 参考IBM developerWorks 转载请注明出处，谢谢。","categories":[{"name":"Java NIO","slug":"Java-NIO","permalink":"http://blog.tifosi-m.com/categories/Java-NIO/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://blog.tifosi-m.com/tags/Java/"}],"keywords":[{"name":"Java NIO","slug":"Java-NIO","permalink":"http://blog.tifosi-m.com/categories/Java-NIO/"}]},{"title":"Apache CXF+SSH搭建webservice","slug":"Apache-CXF-SSH-webservice","date":"2014-04-08T18:22:39.000Z","updated":"2017-09-29T04:58:45.351Z","comments":true,"path":"2014/04/08/Apache-CXF-SSH-webservice/","link":"","permalink":"http://blog.tifosi-m.com/2014/04/08/Apache-CXF-SSH-webservice/","excerpt":"之前几次的项目后台这方面一直由我来负责，经过几次使用也比较熟悉了，趁着刚建了个blog就介绍下webservice环境配置。 ## 准备工作 首先要去Apache 网站下载CXF jar包,structs2、spring、hibernate的库可以自己下载，如果是用myeclipse用集成的也可以。 关于Apache CXF 所必须要使用的Jar包如果嫌麻烦可以直接把libs里的文件全部拷出来，如果和SSH里重复了移除就好，这里就不再多说了。 ## 集成环境 进入正题，当把所有依赖的jar包都准备完成的时候，把所有jar文件复制到WEB-INI/lib目录里即可。 建立spring的配置文件applicationContext.xml：","text":"之前几次的项目后台这方面一直由我来负责，经过几次使用也比较熟悉了，趁着刚建了个blog就介绍下webservice环境配置。 ## 准备工作 首先要去Apache 网站下载CXF jar包,structs2、spring、hibernate的库可以自己下载，如果是用myeclipse用集成的也可以。 关于Apache CXF 所必须要使用的Jar包如果嫌麻烦可以直接把libs里的文件全部拷出来，如果和SSH里重复了移除就好，这里就不再多说了。 ## 集成环境 进入正题，当把所有依赖的jar包都准备完成的时候，把所有jar文件复制到WEB-INI/lib目录里即可。 建立spring的配置文件applicationContext.xml： 内容如下 1234567891011121314151617181920212223242526272829303132333435363738394041&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;beans xmlns=\"http://www.springframework.org/schema/beans\" xmlns:jaxws=\"http://cxf.apache.org/jaxws\" xmlns:soap=\"http://cxf.apache.org/bindings/soap\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns:p=\"http://www.springframework.org/schema/p\" xsi:schemaLocation=\" http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-3.1.xsd http://cxf.apache.org/bindings/soap http://cxf.apache.org/schemas/configuration/soap.xsd http://cxf.apache.org/jaxws http://cxf.apache.org/schemas/jaxws.xsd\"&gt; &lt;import resource=\"classpath:META-INF/cxf/cxf.xml\" /&gt; &lt;import resource=\"classpath:META-INF/cxf/cxf-extension-soap.xml\" /&gt; &lt;import resource=\"classpath:META-INF/cxf/cxf-servlet.xml\" /&gt; &lt;bean id=\"dataSource\" class=\"org.springframework.jdbc.datasource.DriverManagerDataSource\"&gt; &lt;property name=\"driverClassName\" value=\"com.mysql.jdbc.Driver\"&gt; &lt;/property&gt; &lt;property name=\"url\" value=\"jdbc:mysql://localhost:3306/memo\"&gt;&lt;/property&gt; &lt;property name=\"username\" value=\"root\"&gt;&lt;/property&gt; &lt;property name=\"password\" value=\"root\"&gt;&lt;/property&gt; &lt;/bean&gt; &lt;bean id=\"sessionFactory\" class=\"org.springframework.orm.hibernate3.LocalSessionFactoryBean\"&gt; &lt;property name=\"dataSource\"&gt; &lt;ref bean=\"dataSource\" /&gt; &lt;/property&gt; &lt;property name=\"hibernateProperties\"&gt; &lt;props&gt; &lt;prop key=\"hibernate.dialect\"&gt; org.hibernate.dialect.MySQLDialect &lt;/prop&gt; &lt;/props&gt; &lt;/property&gt; &lt;!--这里主要是hibernate的映射文件的地方 --&gt; &lt;property name=\"mappingResources\"&gt; &lt;list&gt; &lt;value&gt;domain/SystemManagement/Account.hbm.xml&lt;/value&gt; &lt;/list&gt; &lt;/property&gt; &lt;/bean&gt; 数据库源部分根据各自情况配置吧主要就是url、username以及password PS:因为这里使用spring来管理hibernate所以hibernate的配置文件可以省略了不用单独再配置了。 接下来在web.xml里配置spring的监听器以及启动CXF的servelt代码如下： 1234567891011121314151617181920212223242526272829&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;web-app version=\"3.0\" xmlns=\"http://java.sun.com/xml/ns/javaee\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://java.sun.com/xml/ns/javaee http://java.sun.com/xml/ns/javaee/web-app_3_0.xsd\"&gt; &lt;display-name&gt;webservice_cxf&lt;/display-name&gt; &lt;context-param&gt; &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;param-value&gt;classpath:applicationContext.xml&lt;/param-value&gt; &lt;/context-param&gt; &lt;listener&gt; &lt;listener-class&gt;org.springframework.web.context.ContextLoaderListener&lt;/listener-class&gt; &lt;/listener&gt; &lt;!-- CXF Servlet --&gt; &lt;servlet&gt; &lt;servlet-name&gt;CXFServlet&lt;/servlet-name&gt; &lt;servlet-class&gt;org.apache.cxf.transport.servlet.CXFServlet&lt;/servlet-class&gt; &lt;load-on-startup&gt;1&lt;/load-on-startup&gt; &lt;/servlet&gt; &lt;servlet-mapping&gt; &lt;servlet-name&gt;CXFServlet&lt;/servlet-name&gt; &lt;url-pattern&gt;/*&lt;/url-pattern&gt; &lt;/servlet-mapping&gt; &lt;welcome-file-list&gt; &lt;welcome-file&gt;index.jsp&lt;/welcome-file&gt; &lt;/welcome-file-list&gt;&lt;/web-app&gt; 环境配置到此结束下面就是实现功能。 ## 功能实现 首先建立domain包用于放实体，这里我以简单的登陆用例说明 Account.java 12345678910111213141516171819202122232425262728293031323334353637383940414243public abstract class AbstractAccount implements java.io.Serializable &#123; // Fields private String tel; private String pwd; // Constructors /** default constructor */ public AbstractAccount() &#123; &#125; /** minimal constructor */ public AbstractAccount(String tel) &#123; this.tel = tel; &#125; /** full constructor */ public AbstractAccount(String tel, String pwd) &#123; this.tel = tel; this.pwd = pwd; &#125; // Property accessors public String getTel() &#123; return this.tel; &#125; public void setTel(String tel) &#123; this.tel = tel; &#125; public String getPwd() &#123; return this.pwd; &#125; public void setPwd(String pwd) &#123; this.pwd = pwd; &#125;&#125; 同时配置好hibernate的映射 1234567891011121314151617&lt;?xml version=\"1.0\" encoding=\"utf-8\"?&gt;&lt;!DOCTYPE hibernate-mapping PUBLIC \"-//Hibernate/Hibernate Mapping DTD 3.0//EN\"\"http://hibernate.sourceforge.net/hibernate-mapping-3.0.dtd\"&gt;&lt;!-- Mapping file autogenerated by MyEclipse Persistence Tools--&gt;&lt;hibernate-mapping&gt; &lt;class name=\"com.demo.domain.Account\" table=\"account\"&gt; &lt;id name=\"tel\" type=\"java.lang.String\"&gt; &lt;column name=\"tel\" length=\"13\" /&gt; &lt;generator class=\"assigned\" /&gt; &lt;/id&gt; &lt;property name=\"pwd\" type=\"java.lang.String\"&gt; &lt;column name=\"pwd\" length=\"17\" /&gt; &lt;/property&gt; &lt;/class&gt;&lt;/hibernate-mapping&gt; 再建立service用以处理业务逻辑 12345678910111213package com.demo.service;import javax.jws.WebParam;import javax.jws.WebService;@WebServicepublic interface ISystemManagementService &#123; public boolean login(@WebParam(name = \"tel\") String tel, @WebParam(name = \"pwd\") String pwd); public boolean register(@WebParam(name = \"tel\") String tel, @WebParam(name = \"pwd\") String pwd);&#125; 1234567891011121314151617181920212223242526272829303132333435363738394041package com.demo.service;import org.springframework.context.ApplicationContext;import org.springframework.context.support.ClassPathXmlApplicationContext;import com.demo.domain.Account;import com.demo.domain.AccountDAO;public class SystemManagementService implements ISystemManagementService &#123; private AccountDAO accountDAO; private ApplicationContext ctx; @Override public boolean login(String tel, String pwd) &#123; ctx = new ClassPathXmlApplicationContext(\"applicationContext.xml\"); accountDAO = AccountDAO.getFromApplicationContext(ctx); Account account = accountDAO.findById(tel); if(account!=null)&#123; if(account.getPwd().equals(pwd)) return true; &#125; return false; &#125; @Override public boolean register(String tel, String pwd) &#123; ctx = new ClassPathXmlApplicationContext(\"applicationContext.xml\"); accountDAO = AccountDAO.getFromApplicationContext(ctx); Account account = new Account(); account.setTel(tel); account.setPwd(pwd); try &#123; accountDAO.save(account); return true; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; return false; &#125;&#125; 写完后在spring的配置文件中配置CXF的bean就可以正常利用spring注入数据源等 123456789&lt;!-- SystemManagerment --&gt; &lt;bean id=\"SystemManagement\" class=\"com.demo.service.SystemManagementService\"&gt; &lt;/bean&gt; &lt;jaxws:server id=\"management\" serviceClass=\"com.demo.service.ISystemManagementService\" address=\"/SystemManagement\"&gt; &lt;jaxws:serviceBean&gt; &lt;ref bean=\"SystemManagement\" /&gt; &lt;/jaxws:serviceBean&gt; &lt;/jaxws:server&gt; 当用localhost访问出现wsdl文档是说明webservice搭建成功。 关于structs2的集成只需要在structs2的配置文件里加上 1&lt;constant name=\"struts.objectFactory\" value=\"spring\" /&gt; 在web.xml里加上 12345678910filter&gt; &lt;filter-name&gt;struts2&lt;/filter-name&gt; &lt;filter-class&gt; org.apache.struts2.dispatcher.ng.filter.StrutsPrepareAndExecuteFilter &lt;/filter-class&gt; &lt;/filter&gt; &lt;filter-mapping&gt; &lt;filter-name&gt;struts2&lt;/filter-name&gt; &lt;url-pattern&gt;/*&lt;/url-pattern&gt; &lt;/filter-mapping&gt; 即可，至于struct2的用法这里就不说明啦 转载请注明出处，谢谢。","categories":[{"name":"Java Web","slug":"Java-Web","permalink":"http://blog.tifosi-m.com/categories/Java-Web/"}],"tags":[{"name":"Apache CXF","slug":"Apache-CXF","permalink":"http://blog.tifosi-m.com/tags/Apache-CXF/"},{"name":"Webservice","slug":"Webservice","permalink":"http://blog.tifosi-m.com/tags/Webservice/"},{"name":"Java","slug":"Java","permalink":"http://blog.tifosi-m.com/tags/Java/"}],"keywords":[{"name":"Java Web","slug":"Java-Web","permalink":"http://blog.tifosi-m.com/categories/Java-Web/"}]}]}