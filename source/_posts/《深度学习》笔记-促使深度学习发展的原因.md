---
title: 《Deep Learning》—— 促使深度学习发展的原因
date: 2017-08-19 19:30:00
categories:
- Deep Learning Book
tags:
- Deep Learning
comments: true
mathjax: true
---

### 1. 维数灾难

一组变量不同的可能配置数量会随着变量的数目的增长而指数级增长，而这种可能的配置数量远大于训练样本的数目。许多传统机器学习算法只是简单的假设在一个新点的输出大致和最接近点的输出相同。

### 2. 局部不变性和平滑正则化

传统机器学习算法中存在一个使用最广泛的隐式"先验"：平滑先验或称为局部不变性先验。该先验表明学习的函数不应在小区域内发生很大的变化。

许多不同的方法显式或隐式地表示学习函数应具有平滑或局部先验，其旨在鼓励学习过程能学习出函数$f^*$，对于大多数$x$和小变动$\epsilon$都满足条件:
$$
f^*\approx f^*(x+\epsilon)
$$

1) 局部不变方法的一个极端例子是k-最近邻系列算法。

2) 大部分核机器也是和附件训练样本相关的训练集上输出插值。

3) 决策树也具有平滑学习的局限性，因为它将输入空间分成和叶节点一样多的区间，并在每个区间使用单独的参数。
<!-- more -->
只要在要学习的真实函数的峰值和谷值处有足够多的样本，那么平滑性假设和相关的无参数学习算法效果都很好。当要学习的函数足够平滑，并且只在少数几维变化时，这样的方法一般没有问题。但在高维空间上，即使非常平滑的函数，也会在不同的维度上有不同的变化方式。如果在不同的区间中表现的不一样，将非常难以用一组训练样本去刻画函数。为了解决这个问题，只有通过额外假设生成数据的分布来建立区域的依赖关系，那么$O(k)$个样本足以描述$O(2^k)$的大量区间。

一般的机器学习算法往往针对特定的问题提出特定的假设，而由于人工智能任务通常较为复杂，因此希望学习算法提出更通用的假设。深度学习的核心思想是假设数据由因素或特征组合参数，这些因素或特征组合可能来自一个层次结构的多个层级，许多类似的通用假设进一步提高了深度学习算法。
