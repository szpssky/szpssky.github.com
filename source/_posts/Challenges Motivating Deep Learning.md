---
title: 《Deep Learning》—— 促使深度学习发展的原因
date: 2017-08-19 19:30:00
categories:
- Deep Learning Book
tags:
- Deep Learning
comments: true
mathjax: true
---

### 1. 维数灾难

一组变量不同的可能配置数量会随着变量的数目的增长而指数级增长，而这种可能的配置数量远大于训练样本的数目。许多传统机器学习算法只是简单的假设在一个新点的输出大致和最接近点的输出相同。

### 2. 局部不变性和平滑正则化

传统机器学习算法中存在一个使用最广泛的隐式"先验"：平滑先验或称为局部不变性先验。该先验表明学习的函数不应在小区域内发生很大的变化。

许多不同的方法显式或隐式地表示学习函数应具有平滑或局部先验，其旨在鼓励学习过程能学习出函数$f^*$，对于大多数$x$和小变动$\epsilon$都满足条件:
$$
f^*\approx f^*(x+\epsilon)
$$

1) 局部不变方法的一个极端例子是k-最近邻系列算法，当一个区域内的所有点$x$在训练集中的$k$个近邻的结果是一样的，则对这些点的预测也是一样的。

2) 大部分核机器也是和附近训练样本相关的训练集的输出进行插值，例如一类重要的核函数是局部核(local kernel)。

3) 决策树也具有平滑学习的局限性，因为它将输入空间分成和叶节点一样多的区间，并在每个区间使用单独的参数。
<!-- more -->
只要在要学习的真实函数的峰值和谷值处有足够多的样本，那么平滑性假设和相关的无参数学习算法效果都很好。通常当要学习的函数足够平滑，并且只在少数几维变化时，这样的方法一般没有问题。但在高维空间上，即使非常平滑的函数，也会在不同的维度上有不同的变化方式。如果函数在不同的区间中表现的不一样，那么将非常难以用一组训练样本去刻画这个函数。为了解决这个问题，需要额外假设生成数据的分布，依次来建立区域间的依赖关系，那么$O(k)$个样本是足以描述多如$O(2^k)$的大量区间。

一般的，机器学习算法往往提出更强的、针对特定问题的假设针例如假设目标函数是周期性的，而由于人工智能任务通常较为复杂，很难限制到简单的、人工指定的性质，因此希望学习算法具有更通用的假设。深度学习的核心思想是假设数据是由因素或特征组合产生，这些因素或特征组合可能来自一个层次结构的多个层级。深度的分布式表示带来的指数增溢有效的解决了维数灾难问题。
